{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression part a:\n",
    "In this section, you are to solve a relevant regression problem\n",
    "for your data and statistically evaluate the result. We will begin by examining the\n",
    "most elementary model, namely linear regression.\n",
    "\n",
    "1:\n",
    "\n",
    "Explain what variable is predicted based on which other variables and what\n",
    "you hope to accomplish by the regression. Mention your feature transformation\n",
    "choices such as one-of-K coding. Since we will use regularization momentarily,\n",
    "apply a feature transformation to your data matrix X such that each column\n",
    "has mean 0 and standard deviation 1 3 .\n",
    "\n",
    "Since out desired output variable is a boolean we cant do regression on it so we have therefore picked age instead to try and see if linear regression can estimate the subjects age based on the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chd', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read data\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import sys\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 7.5})\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as lm\n",
    "from toolbox_02450 import rlr_validate\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\"\"\"\n",
    "Explanation of variables in dataset SAHD.xls (african heart disease)\n",
    "chd: Coronary heart disease (bool (Actually its a string but its only used in the dictionary and as whether or not the subject has the disease)) \n",
    "sbp: Systolic blood pressure (int)\n",
    "tobacco: tobacco in kg (float)\n",
    "ldl: ? (float)\n",
    "adiposity: ? (float)\n",
    "famhist: Family history of CHD (bool (Present, Absent)) (string)\n",
    "typea: ? (int)\n",
    "obesity: ? (float)\n",
    "alcohol: alcohol consumption in liters (float)\n",
    "age: age (int)\n",
    "\"\"\"\n",
    "\n",
    "# Load xls sheet with data\n",
    "doc = xlrd.open_workbook('Data/SAHD2.xls').sheet_by_index(0)\n",
    "\n",
    "# Extract attribute names (1st row, column 4 to 12)\n",
    "attributeNames = doc.row_values(0, 0, 10)\n",
    "print(attributeNames)\n",
    "\n",
    "# Extract class names to python list,\n",
    "# then encode with integers (dict)\n",
    "classLabels = doc.col_values(0, 1, 463)\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = dict(zip(classNames, range(2)))\n",
    "\n",
    "# Extract vector y, convert to NumPy array\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Preallocate memory, then extract excel data to matrix X\n",
    "X = np.empty((462, 9))\n",
    "for i, col_id in enumerate(range(1,10)):\n",
    "    X[:, i] = np.asarray(doc.col_values(col_id, 1, 463))\n",
    "# Compute values of N, M and C.\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)\n",
    "\n",
    "#print(X)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "Y2 = X - np.ones((N, 1))*X.mean(0)\n",
    "X = (Y2*(1/np.std(Y2,0)))\n",
    "\n",
    "X_without_age = np.delete(X,8,axis=1)\n",
    "\n",
    "# print(Y2_without_age)\n",
    "# print(Y2)\n",
    "eps_mean, eps_std = 0, 0.1\n",
    "eps = np.array(eps_std*np.random.randn(N) + eps_mean).reshape(-1,1)\n",
    "w0 = -0.5\n",
    "w1 = 0.01\n",
    "y = w0 + w1*X[:,[8]] + eps\n",
    "y_true = y - eps\n",
    "\n",
    "# Fit ordinary least squares regression model\n",
    "model = lm.LinearRegression(fit_intercept=True)\n",
    "model = model.fit(X,y)\n",
    "# Compute model output:\n",
    "y_est = model.predict(X)\n",
    "# Or equivalently:\n",
    "#y_est = model.intercept_ + X @ model.coef_\n",
    "\n",
    "# Plot original data and the model output\n",
    "f = plt.figure()\n",
    "\n",
    "plt.plot(y_est,y_true,'.')\n",
    "plt.xlim([-0.55, -0.45])\n",
    "plt.ylim([-0.55, -0.45])\n",
    "\n",
    "plt.xlabel('y_est'); plt.ylabel('y_true')\n",
    "plt.legend(['Training data', 'Data generator', 'Regression fit (model)'])\n",
    "\n",
    "\n",
    "### Implementer ting fra ex8. \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:\n",
    "\n",
    "Introduce a regularization parameter λ as discussed in chapter 14 of the lecture\n",
    "notes, and estimate the generalization error for different values of λ. Specifi-\n",
    "cally, choose a reasonable range of values of λ (ideally one where the general-\n",
    "ization error first drop and then increases), and for each value use K = 10 fold\n",
    "cross-validation (algorithm 5) to estimate the generalization error.\n",
    "Include a figure of the estimated generalization error as a function of λ in the\n",
    "report and briefly discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 8\n",
      "N: 415 M: 8\n",
      "N: 415 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n",
      "N: 416 M: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression without feature selection:\n",
      "- Training error: 0.454257931209123\n",
      "- Test error:     0.4741292700220841\n",
      "- R^2 train:     0.545599532762653\n",
      "- R^2 test:     0.5137176140364939\n",
      "\n",
      "Regularized linear regression:\n",
      "- Training error: 0.4549180746922052\n",
      "- Test error:     0.47498902694839434\n",
      "- R^2 train:     0.5449391821412389\n",
      "- R^2 test:     0.5128358193954345\n",
      "\n",
      "Weights in last fold:\n",
      "            chd            0.17\n",
      "            sbp            0.23\n",
      "        tobacco            0.06\n",
      "            ldl             0.6\n",
      "      adiposity            0.11\n",
      "        famhist           -0.05\n",
      "          typea           -0.23\n",
      "        obesity           -0.01\n",
      "Ran Exercise 8.1.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# exercise 8.1.1\n",
    "\n",
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import rlr_validate\n",
    "\n",
    "attributeNames = ['chd', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    "\n",
    "X_only_age = X[:,[8]].squeeze()\n",
    "\n",
    "N, M = X_without_age.shape\n",
    "print(N,M)\n",
    "\n",
    "# Add offset attribute\n",
    "#X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "#attributeNames = [u'Offset']+attributeNames\n",
    "#M = M+1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "w_noreg = np.empty((M,K))\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X_without_age, X_only_age):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X_without_age[train_index]\n",
    "    y_train = X_only_age[train_index]\n",
    "    X_test = X_without_age[test_index]\n",
    "    y_test = X_only_age[test_index]\n",
    "\n",
    "    \n",
    "    N, M = X_train.shape\n",
    "    print(f'N: {N} M: {M}')\n",
    "\n",
    "    internal_cross_validation = 10    \n",
    "    \n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    \n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    \n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = np.square(y_train-y_train.mean()).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_nofeatures[k] = np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = np.square(y_train-X_train @ w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_rlr[k] = np.square(y_test-X_test @ w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:,k] = np.linalg.solve(XtX,Xty).squeeze()\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    #m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    # Display the results for the last cross-validation fold\n",
    "    if k == K-1:\n",
    "        figure(k, figsize=(12,8))\n",
    "        subplot(1,2,1)\n",
    "        semilogx(lambdas,mean_w_vs_lambda.T[:,1:],'.-') # Don't plot the bias term\n",
    "        xlabel('Regularization factor')\n",
    "        ylabel('Mean Coefficient Values')\n",
    "        grid()\n",
    "        # You can choose to display the legend, but it's omitted for a cleaner \n",
    "        # plot, since there are many attributes\n",
    "        #legend(attributeNames[1:], loc='best')\n",
    "        \n",
    "        subplot(1,2,2)\n",
    "        title('Optimal lambda: 1e{0}'.format(np.log10(opt_lambda)))\n",
    "        loglog(lambdas,train_err_vs_lambda.T,'b.-',lambdas,test_err_vs_lambda.T,'r.-')\n",
    "        xlabel('Regularization factor')\n",
    "        ylabel('Squared error (crossvalidation)')\n",
    "        legend(['Train error','Validation error'])\n",
    "        grid()\n",
    "    \n",
    "    # To inspect the used indices, use these print statements\n",
    "    #print('Cross validation fold {0}/{1}:'.format(k+1,K))\n",
    "    #print('Train indices: {0}'.format(train_index))\n",
    "    #print('Test indices: {0}\\n'.format(test_index))\n",
    "\n",
    "    k+=1\n",
    "\n",
    "show()\n",
    "# Display results\n",
    "print('Linear regression without feature selection:')\n",
    "print('- Training error: {0}'.format(Error_train.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test.sum())/Error_test_nofeatures.sum()))\n",
    "print('Regularized linear regression:')\n",
    "print('- Training error: {0}'.format(Error_train_rlr.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test_rlr.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train_rlr.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_rlr.sum())/Error_test_nofeatures.sum()))\n",
    "\n",
    "print('Weights in last fold:')\n",
    "for m in range(M):\n",
    "    print('{:>15} {:>15}'.format(attributeNames[m], np.round(w_rlr[m,-1],2)))\n",
    "\n",
    "print('Ran Exercise 8.1.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3: Jeg forstår ikke spørgsmålet..\n",
    "3:\n",
    "\n",
    "Explain how a new data observation is predicted according to the linear model\n",
    "with the lowest generalization error as estimated in the previous question. I.e.,\n",
    "what are the effects of the selected attributes in terms of determining the\n",
    "predicted class. Does the result make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression part b:\n",
    "In this section, we will compare three models: the regularized\n",
    "linear regression model from the previous section, an artificial neural network (ANN)\n",
    "and a baseline. We are interested in two questions: Is one model better than the\n",
    "other? Is either model better than a trivial baseline?. We will attempt to answer\n",
    "these questions with two-level cross-validation.\n",
    "\n",
    "1:\n",
    "\n",
    "Implement two-level cross-validation (see algorithm 6 of the lecture notes). We\n",
    "will use 2-level cross-validation to compare the models with K 1 = K 2 = 10 \n",
    "folds. As a baseline model, we will apply a linear regression model with no\n",
    "features, i.e. it computes the mean of y on the training data, and use this value\n",
    "to predict y on the test data.\n",
    "Make sure you can fit an ANN model to the data. As complexity-controlling\n",
    "parameter for the ANN, we will use the number of hidden units 5 h. Based on\n",
    "a few test-runs, select a reasonable range of values for h (which should include\n",
    "h = 1), and describe the range of values you will use for h and λ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9088653\t7.285561e-05\n",
      "\t\t2000\t0.8203442\t6.175556e-05\n",
      "\t\t3000\t0.76852614\t2.8307533e-05\n",
      "\t\t4000\t0.7615234\t6.8877366e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4161\t0.7606279\t5.4853666e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7647984\t2.4159299e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1667\t0.75799334\t1.5726955e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7830797\t0.000112942915\n",
      "\t\t2000\t0.7649486\t6.467302e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2270\t0.7640667\t9.36116e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76127744\t2.4975661e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1712\t0.75114274\t6.3481525e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7707887\t2.4048872e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1550\t0.7640403\t5.4608677e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7720637\t2.9335793e-05\n",
      "\t\t2000\t0.7584346\t1.1159519e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2738\t0.75024945\t3.1778563e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7504248\t2.4145489e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1434\t0.7458458\t8.7906983e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7523336\t2.0281535e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1645\t0.74595594\t3.9951868e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75558066\t1.7591241e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1784\t0.7402558\t3.2207606e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76018554\t6.22521e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1772\t0.7428698\t8.023565e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7748499\t3.2307085e-05\n",
      "\t\t2000\t0.75595367\t1.3009579e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2805\t0.7491122\t7.9566985e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74553245\t3.8853752e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1854\t0.73173445\t0.0\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.83746314\t2.96782e-05\n",
      "\t\t2000\t0.77127427\t4.2193504e-05\n",
      "\t\t3000\t0.7442731\t2.5866582e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3536\t0.7350544\t3.2435491e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7585977\t2.8913713e-05\n",
      "\t\t2000\t0.7453993\t9.355627e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2840\t0.7403614\t1.6101498e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75016165\t3.6945552e-05\n",
      "\t\t2000\t0.725382\t1.8077075e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2328\t0.7209478\t4.133768e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75002474\t3.6396046e-05\n",
      "\t\t2000\t0.7377721\t1.4945936e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2230\t0.7357599\t4.0505483e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75141007\t3.0300744e-05\n",
      "\t\t2000\t0.7361715\t1.1496998e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2269\t0.7334219\t8.1269235e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75869775\t3.79439e-05\n",
      "\t\t2000\t0.73229414\t2.9301127e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2443\t0.7249908\t5.755001e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.741563\t5.7144814e-05\n",
      "\t\t2000\t0.7188157\t1.7910534e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2023\t0.7185547\t8.2950675e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7339236\t3.9468294e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1491\t0.72124195\t0.0\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7716787\t3.939097e-05\n",
      "\t\t2000\t0.74984926\t2.1461525e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2960\t0.73333013\t9.75352e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7508482\t2.468753e-05\n",
      "\t\t2000\t0.73349875\t3.0228079e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2308\t0.72881377\t3.2713248e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74623823\t4.4567416e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1868\t0.72336376\t9.0639116e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75126517\t2.5863852e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1737\t0.73974\t4.028758e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7225808\t4.652138e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1903\t0.70118475\t8.500562e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7431039\t5.12518e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1864\t0.7165855\t2.4953601e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7628517\t3.8440445e-05\n",
      "\t\t2000\t0.74093914\t2.807442e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2852\t0.7185409\t1.6590464e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.736007\t4.86689e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1702\t0.717028\t1.662547e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7498018\t3.8712013e-05\n",
      "\t\t2000\t0.7339034\t2.1684182e-05\n",
      "\t\t3000\t0.71715486\t2.51825e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3999\t0.700739\t8.5059696e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7257657\t2.5130097e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1120\t0.72220206\t8.253182e-08\n",
      "Cross validation fold 1/10\n",
      "[0.401] m1 (regression) test error\n",
      "[[9.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.7007390260696411 m2 (neural network) error\n",
      "[1.057] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.402] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73545796\t2.7392209e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1288\t0.73142827\t4.0745363e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7472887\t3.5173452e-05\n",
      "\t\t2000\t0.7372\t9.37883e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2873\t0.73221076\t5.698254e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75127923\t2.7767368e-05\n",
      "\t\t2000\t0.7424647\t1.4289534e-05\n",
      "\t\t3000\t0.7318557\t1.0017409e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3215\t0.7303232\t4.0807015e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7688455\t3.6822956e-05\n",
      "\t\t2000\t0.75356764\t7.751408e-06\n",
      "\t\t3000\t0.7411731\t1.5842357e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3991\t0.72846144\t4.9093615e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7377161\t1.4947071e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1877\t0.7226269\t8.2483353e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8411743\t0.00032435745\n",
      "\t\t2000\t0.76370645\t1.3267736e-05\n",
      "\t\t3000\t0.74176055\t4.7487927e-05\n",
      "\t\t4000\t0.7274641\t1.9663972e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4471\t0.7246917\t9.0473026e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74787253\t2.6140568e-05\n",
      "\t\t2000\t0.7281488\t1.5552736e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2687\t0.7158692\t9.991421e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7435288\t2.693456e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1265\t0.7406811\t8.85201e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7837145\t0.00020347917\n",
      "\t\t2000\t0.7223566\t2.9291667e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2694\t0.71448225\t9.176582e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73778397\t2.7709773e-05\n",
      "\t\t2000\t0.7229127\t1.7891478e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2923\t0.71078736\t1.6771445e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7461201\t2.6122081e-05\n",
      "\t\t2000\t0.73052704\t2.3089802e-05\n",
      "\t\t3000\t0.7091061\t2.5216174e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3014\t0.70890015\t5.885635e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73110455\t2.5924863e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1775\t0.7100199\t2.5184363e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7337967\t2.672321e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1974\t0.6963601\t4.2797305e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74406946\t4.269483e-05\n",
      "\t\t2000\t0.7248999\t2.1377953e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2512\t0.71871483\t2.4879685e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73595995\t3.830632e-05\n",
      "\t\t2000\t0.7173981\t3.0158755e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2839\t0.6981841\t7.6833805e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7147458\t2.7935803e-05\n",
      "\t\t2000\t0.6890864\t2.1451062e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2239\t0.68530154\t9.567347e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74500513\t4.608115e-05\n",
      "\t\t2000\t0.71836495\t2.0161948e-05\n",
      "\t\t3000\t0.70645577\t9.955723e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3559\t0.70038116\t4.255156e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74530363\t5.3019714e-05\n",
      "\t\t2000\t0.7127103\t3.9723127e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2994\t0.6945197\t7.723919e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.71548086\t4.073552e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1977\t0.6886684\t6.924051e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7041354\t5.3664844e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1395\t0.6894339\t2.5936336e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73744565\t9.851696e-05\n",
      "\t\t2000\t0.71000373\t1.7377299e-05\n",
      "\t\t3000\t0.6926862\t5.25729e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3534\t0.6804026\t8.760195e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7089661\t8.263657e-05\n",
      "\t\t2000\t0.66716385\t2.501467e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2008\t0.6669694\t7.1493156e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6925575\t4.1653504e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1159\t0.68493277\t3.4809037e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7174824\t3.040442e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1556\t0.70220923\t0.0\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69803333\t3.0056164e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1458\t0.68224037\t9.610256e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7110578\t4.4509263e-05\n",
      "\t\t2000\t0.6856619\t2.8512253e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2562\t0.67340225\t5.310764e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7260856\t4.83489e-05\n",
      "\t\t2000\t0.7005735\t4.0751558e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2137\t0.6979005\t8.5405645e-08\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t968\t0.69244415\t1.7215723e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7138913\t4.8256396e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1711\t0.6857016\t6.953999e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.71871394\t4.6937515e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1857\t0.6950758\t2.5725825e-07\n",
      "Cross validation fold 2/10\n",
      "[0.622] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6857016086578369 m2 (neural network) error\n",
      "[-1.913] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.609] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7872255\t7.472496e-05\n",
      "\t\t2000\t0.76841855\t1.6211437e-05\n",
      "\t\t3000\t0.75891876\t8.953354e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3323\t0.7573306\t3.148145e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76976264\t1.9125462e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1893\t0.75901\t8.6382477e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7836648\t1.9318575e-05\n",
      "\t\t2000\t0.76588404\t1.1206622e-05\n",
      "\t\t3000\t0.76044816\t8.229931e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3599\t0.75702024\t9.4482954e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77665305\t2.1258058e-05\n",
      "\t\t2000\t0.75398403\t2.1264785e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2149\t0.75219566\t4.7544503e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7745593\t3.3550372e-05\n",
      "\t\t2000\t0.76132923\t7.3592278e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2585\t0.7570433\t3.936675e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8209542\t0.00021631343\n",
      "\t\t2000\t0.7772845\t2.8755365e-05\n",
      "\t\t3000\t0.7654822\t9.655228e-06\n",
      "\t\t4000\t0.75809246\t8.4913745e-06\n",
      "\t\t5000\t0.7484842\t8.998539e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5340\t0.7454574\t8.795278e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7764873\t2.8861643e-05\n",
      "\t\t2000\t0.7536824\t2.9734912e-05\n",
      "\t\t3000\t0.7406774\t1.3841191e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3307\t0.73816174\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7776639\t2.169028e-05\n",
      "\t\t2000\t0.7550102\t1.9893867e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2419\t0.7479833\t3.9843584e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8697617\t0.0007038541\n",
      "\t\t2000\t0.76765496\t5.357483e-06\n",
      "\t\t3000\t0.7589391\t4.499956e-05\n",
      "\t\t4000\t0.7388643\t2.0489919e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4888\t0.7303819\t2.448226e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7529399\t6.126807e-05\n",
      "\t\t2000\t0.72701806\t3.0497526e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2440\t0.7189283\t8.2907707e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75690734\t8.244195e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1434\t0.740093\t5.6375717e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7471728\t2.0182308e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1313\t0.74258196\t8.0266693e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7565871\t2.8754217e-05\n",
      "\t\t2000\t0.7389503\t2.2826609e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2542\t0.7310826\t2.445878e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7458017\t6.169461e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1712\t0.73086095\t8.1554006e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.766625\t3.358662e-05\n",
      "\t\t2000\t0.75318027\t9.733792e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2223\t0.751427\t5.5525385e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76449835\t7.01642e-05\n",
      "\t\t2000\t0.74032104\t1.3767343e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2330\t0.73679286\t9.707701e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7425498\t2.3117293e-05\n",
      "\t\t2000\t0.7148866\t2.9014132e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2100\t0.71318555\t9.193267e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t986\t0.74492353\t1.6002888e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7394726\t3.9575127e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1281\t0.7331246\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7358733\t4.284638e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1529\t0.720511\t3.3090217e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74647534\t4.2078183e-05\n",
      "\t\t2000\t0.7226782\t2.5402405e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2099\t0.7209848\t5.786984e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7460924\t6.2709005e-05\n",
      "\t\t2000\t0.71341604\t3.2916916e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2101\t0.7107384\t9.2249195e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7405196\t4.966005e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1897\t0.7062674\t8.439387e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7537147\t2.8468414e-05\n",
      "\t\t2000\t0.72946095\t2.2878427e-05\n",
      "\t\t3000\t0.7120582\t1.7243457e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3600\t0.702743\t5.0890304e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.756498\t8.193512e-05\n",
      "\t\t2000\t0.7242117\t3.7446367e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2316\t0.71818835\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7735207\t7.697332e-05\n",
      "\t\t2000\t0.73774445\t1.9228382e-05\n",
      "\t\t3000\t0.7240481\t1.3418206e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3865\t0.7123423\t5.857195e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7545849\t6.03447e-05\n",
      "\t\t2000\t0.7318725\t3.485566e-05\n",
      "\t\t3000\t0.70850646\t3.3229127e-05\n",
      "\t\t4000\t0.6916152\t1.7580778e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4250\t0.6888763\t3.460977e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7555228\t3.5736764e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1848\t0.7363682\t8.9038565e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75000894\t4.990587e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1797\t0.7312831\t2.4452075e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76512307\t3.458731e-05\n",
      "\t\t2000\t0.7388458\t3.751137e-05\n",
      "\t\t3000\t0.71625906\t4.993021e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3733\t0.7070212\t5.90127e-07\n",
      "Cross validation fold 3/10\n",
      "[0.355] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.707021176815033 m2 (neural network) error\n",
      "[-0.192] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.359] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72390854\t2.1242562e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1911\t0.7187444\t4.9757335e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72582275\t1.4370813e-05\n",
      "\t\t2000\t0.7097894\t1.2428162e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2234\t0.70806444\t7.5761665e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77629346\t0.00016367045\n",
      "\t\t2000\t0.7325536\t3.075524e-05\n",
      "\t\t3000\t0.72302234\t7.914003e-06\n",
      "\t\t4000\t0.7169154\t1.8041144e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4405\t0.7131068\t3.3433793e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72870183\t3.9260372e-05\n",
      "\t\t2000\t0.7152366\t1.2000177e-05\n",
      "\t\t3000\t0.70565933\t1.2585365e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3582\t0.70172256\t1.698809e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72468174\t3.569497e-05\n",
      "\t\t2000\t0.7161263\t8.656056e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2743\t0.7108796\t8.384633e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72098416\t1.0085788e-05\n",
      "\t\t2000\t0.7122451\t1.5397909e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2481\t0.70798755\t6.735102e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7146892\t3.786189e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1485\t0.7063706\t8.438148e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7210809\t5.0254836e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1860\t0.7072859\t4.2136193e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7131663\t1.7885262e-05\n",
      "\t\t2000\t0.704324\t1.1678354e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2167\t0.7029442\t9.327205e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7183722\t9.955626e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1783\t0.7005877\t5.955461e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.71075034\t2.0461806e-05\n",
      "\t\t2000\t0.68988776\t1.8488743e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2842\t0.6800471\t8.764783e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7089431\t4.649151e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1382\t0.698619\t9.384968e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70484\t2.2070923e-05\n",
      "\t\t2000\t0.67632514\t4.0361985e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2176\t0.67304504\t8.855966e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7569999\t0.00019310697\n",
      "\t\t2000\t0.710456\t2.6258858e-05\n",
      "\t\t3000\t0.70116967\t1.4961087e-05\n",
      "\t\t4000\t0.6863312\t1.6500337e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4491\t0.6806918\t7.0051794e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7254159\t3.6726928e-05\n",
      "\t\t2000\t0.7057453\t2.2380422e-05\n",
      "\t\t3000\t0.68766916\t2.1061907e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3709\t0.67952603\t5.262904e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6937219\t3.9178023e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1961\t0.67659163\t7.047641e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6910546\t3.424076e-05\n",
      "\t\t2000\t0.6772527\t1.1441099e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2113\t0.6764506\t9.69251e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7129796\t4.4472887e-05\n",
      "\t\t2000\t0.68241334\t2.7163216e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2542\t0.67183197\t6.210374e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7142856\t7.125825e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1933\t0.6867436\t1.7358634e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72631085\t7.254013e-05\n",
      "\t\t2000\t0.6983184\t2.7910155e-05\n",
      "\t\t3000\t0.6858712\t1.2427054e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3200\t0.68405354\t7.8420965e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70223874\t3.7005455e-05\n",
      "\t\t2000\t0.68421113\t2.7178956e-05\n",
      "\t\t3000\t0.6683763\t1.0968807e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3316\t0.66526055\t9.855544e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7458591\t0.00011426408\n",
      "\t\t2000\t0.70389295\t3.4971024e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2947\t0.6885563\t7.7908254e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7171697\t4.3548232e-05\n",
      "\t\t2000\t0.6982972\t3.4227036e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.6779162\t5.275396e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70402247\t3.5895817e-05\n",
      "\t\t2000\t0.68336856\t2.5293686e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2334\t0.67792964\t8.79215e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6983596\t4.7281388e-05\n",
      "\t\t2000\t0.678461\t1.2211379e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2689\t0.6681582\t3.5682967e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6887132\t5.0107014e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1958\t0.6603288\t4.513257e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69392\t5.2565316e-05\n",
      "\t\t2000\t0.6666904\t3.3078304e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2191\t0.6638404\t2.6936277e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7028779\t3.9600433e-05\n",
      "\t\t2000\t0.6793502\t3.0356334e-05\n",
      "\t\t3000\t0.6584552\t2.0276504e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3015\t0.65828156\t7.2436603e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73219275\t7.5701544e-05\n",
      "\t\t2000\t0.6847255\t4.204288e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2448\t0.67452055\t1.7673192e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6934932\t2.3549313e-05\n",
      "\t\t2000\t0.66690725\t3.6821028e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2818\t0.6479359\t1.8398318e-07\n",
      "Cross validation fold 4/10\n",
      "[0.895] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6479359269142151 m2 (neural network) error\n",
      "[0.074] m3 (baseline) test error\n",
      "1.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.889] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.732619\t3.9050465e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1735\t0.72444916\t5.7593036e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7479034\t2.335028e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1646\t0.7375247\t7.2735384e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73581755\t1.6443673e-05\n",
      "\t\t2000\t0.7274387\t3.5233081e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2442\t0.72436976\t0.0\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7336942\t1.803475e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1727\t0.72816\t7.367081e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72065425\t1.1827257e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1234\t0.71810937\t1.660044e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7369925\t1.2050306e-05\n",
      "\t\t2000\t0.72383523\t2.1574082e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2326\t0.7167546\t9.147489e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74114007\t3.425899e-05\n",
      "\t\t2000\t0.71916413\t2.4863515e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2355\t0.7150565\t4.1678294e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7206453\t3.2173193e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1216\t0.71652454\t2.4955725e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7229173\t2.0612113e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1609\t0.71746325\t3.3230762e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7344752\t3.2216583e-05\n",
      "\t\t2000\t0.71868443\t1.2523144e-05\n",
      "\t\t3000\t0.7072363\t8.680586e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3064\t0.70673805\t5.903641e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7283715\t3.1995583e-05\n",
      "\t\t2000\t0.7134408\t1.1779753e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2657\t0.7078613\t0.0\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t726\t0.72696865\t8.1990737e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.720068\t2.9633078e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1895\t0.7044526\t5.07668e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7153969\t3.9823888e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1429\t0.6998663\t7.664924e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72652286\t4.3889995e-05\n",
      "\t\t2000\t0.70311445\t2.314231e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2529\t0.6944387\t5.1498864e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7185205\t3.6830523e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1696\t0.7075619\t1.6847898e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.71537846\t3.51594e-05\n",
      "\t\t2000\t0.69166934\t3.03327e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2495\t0.68369174\t8.7180507e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69708246\t2.9156652e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1010\t0.69693947\t7.6971014e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7022966\t5.058058e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1454\t0.68844956\t4.3289023e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70678765\t5.262025e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1594\t0.6878885\t6.065413e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69351035\t4.29713e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1554\t0.6764365\t0.0\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.718652\t4.0804574e-05\n",
      "\t\t2000\t0.69308156\t2.6917109e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2272\t0.6874456\t3.4681824e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.705609\t3.2351967e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1936\t0.68096673\t8.7529526e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7234355\t3.4191136e-05\n",
      "\t\t2000\t0.692036\t2.6957776e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2287\t0.6866789\t5.2080776e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69219756\t5.9584083e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1215\t0.685126\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70773196\t4.5139474e-05\n",
      "\t\t2000\t0.67794645\t4.2990738e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2195\t0.67457664\t8.835859e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72081596\t4.6883324e-05\n",
      "\t\t2000\t0.69447356\t1.7336772e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2397\t0.68696463\t4.3382633e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7212834\t3.5284724e-05\n",
      "\t\t2000\t0.6933938\t3.1804462e-05\n",
      "\t\t3000\t0.6760976\t2.8210348e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3049\t0.6754116\t8.824937e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7142168\t4.94861e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1867\t0.68624234\t9.55423e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70429647\t5.6021952e-05\n",
      "\t\t2000\t0.66900915\t3.590356e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2389\t0.6615363\t9.911028e-07\n",
      "Cross validation fold 5/10\n",
      "[0.322] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6615362763404846 m2 (neural network) error\n",
      "[-0.697] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.317] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.79692215\t7.7255776e-05\n",
      "\t\t2000\t0.7734026\t2.1886854e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2892\t0.76181126\t3.129629e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77237374\t1.5665413e-05\n",
      "\t\t2000\t0.7592444\t1.1069113e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2371\t0.7559058\t3.942599e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.761389\t8.767741e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1253\t0.759873\t4.7064142e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76728153\t2.8042736e-05\n",
      "\t\t2000\t0.7545606\t1.6588154e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2119\t0.75328785\t7.121344e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.766233\t3.8193033e-05\n",
      "\t\t2000\t0.74922186\t1.38424475e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2218\t0.74703556\t6.3830623e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7582789\t3.3170283e-05\n",
      "\t\t2000\t0.74701434\t1.2925893e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2231\t0.74540484\t3.9981376e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77321017\t3.0756866e-05\n",
      "\t\t2000\t0.75298\t2.7625496e-05\n",
      "\t\t3000\t0.73870176\t1.5088499e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3074\t0.7380151\t5.653438e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7594668\t1.310636e-05\n",
      "\t\t2000\t0.7522512\t6.338762e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2357\t0.75035006\t6.354859e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7587267\t1.5868622e-05\n",
      "\t\t2000\t0.74782634\t8.050025e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2107\t0.7472715\t3.9881505e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7438078\t3.7661783e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1234\t0.7383527\t9.687172e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7935792\t0.00012676729\n",
      "\t\t2000\t0.7627234\t2.1646287e-05\n",
      "\t\t3000\t0.75315434\t7.8348e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3096\t0.75253826\t1.5840963e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7600733\t2.085918e-05\n",
      "\t\t2000\t0.74344325\t2.6456642e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2287\t0.7406384\t3.2190968e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7450912\t3.567714e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1913\t0.7226493\t9.897678e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7418468\t1.831862e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1323\t0.7343419\t8.116736e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75805384\t3.2158026e-05\n",
      "\t\t2000\t0.74072075\t1.01389205e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2042\t0.7402699\t4.831045e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74741256\t2.0893534e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1362\t0.73898596\t5.6460107e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7405422\t2.0443495e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1787\t0.72777057\t2.457009e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73397666\t6.187653e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1680\t0.7143333\t1.6688186e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7438132\t5.2084313e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1393\t0.7323491\t4.0694164e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7536055\t2.3964494e-05\n",
      "\t\t2000\t0.73024833\t1.5263162e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2563\t0.72039604\t3.3095498e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76227564\t3.96423e-05\n",
      "\t\t2000\t0.74051917\t3.702419e-05\n",
      "\t\t3000\t0.72296154\t2.052841e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3432\t0.7162728\t5.825054e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7394081\t3.941737e-05\n",
      "\t\t2000\t0.7106778\t2.6418395e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2226\t0.7071301\t9.2719927e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73811805\t2.4225068e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1766\t0.7213924\t5.7837144e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72572845\t4.993302e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1204\t0.7204151\t8.273652e-08\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7193525\t5.029273e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1186\t0.7149692\t4.168338e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7706649\t0.00023019368\n",
      "\t\t2000\t0.73417795\t1.2015318e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2254\t0.73014873\t2.4490075e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72853523\t3.452447e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1661\t0.70795375\t7.577351e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74547344\t6.116216e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1699\t0.71224844\t8.368512e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73269093\t4.7913047e-05\n",
      "\t\t2000\t0.69599193\t0.0001084083\n",
      "\t\tFinal loss:\n",
      "\t\t2815\t0.67574733\t6.17439e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76936096\t6.770675e-05\n",
      "\t\t2000\t0.74460256\t3.74615e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2934\t0.72257\t1.6497954e-07\n",
      "Cross validation fold 6/10\n",
      "[0.428] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6757473349571228 m2 (neural network) error\n",
      "[-0.242] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.433] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7514714\t2.569813e-05\n",
      "\t\t2000\t0.7374303\t1.1639025e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2655\t0.73213947\t5.6988154e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t948\t0.7396393\t8.864463e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.86771464\t0.0002586254\n",
      "\t\t2000\t0.7478145\t2.6461401e-05\n",
      "\t\t3000\t0.7412002\t3.377477e-06\n",
      "\t\t4000\t0.73264736\t6.752433e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4580\t0.72998106\t3.2660938e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75381446\t4.7202986e-05\n",
      "\t\t2000\t0.7272234\t3.2701748e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2438\t0.7212356\t8.2642416e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7557672\t5.4178287e-05\n",
      "\t\t2000\t0.73466516\t3.0017834e-05\n",
      "\t\t3000\t0.722329\t9.406885e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3214\t0.7211532\t2.479556e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7453078\t3.28679e-05\n",
      "\t\t2000\t0.7322363\t1.0581996e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2718\t0.7273422\t7.375365e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76132894\t0.00011585617\n",
      "\t\t2000\t0.73054856\t8.077234e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2118\t0.7294461\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7537808\t5.4716384e-05\n",
      "\t\t2000\t0.73364013\t1.1130451e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2540\t0.7296085\t8.9863494e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7636222\t0.00013634375\n",
      "\t\t2000\t0.734306\t1.055217e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2282\t0.7322522\t5.697938e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74946487\t6.322204e-05\n",
      "\t\t2000\t0.72714996\t2.0492136e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2016\t0.7269855\t2.4596636e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7512508\t5.291734e-05\n",
      "\t\t2000\t0.73122734\t1.28789125e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2652\t0.7263366\t9.847432e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73927814\t3.2329768e-05\n",
      "\t\t2000\t0.7103358\t3.22206e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2536\t0.704798\t4.2284896e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7274265\t2.6793354e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1010\t0.7273041\t4.0976408e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7230638\t2.514157e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1132\t0.7209959\t8.266995e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7355696\t3.8164595e-05\n",
      "\t\t2000\t0.7165289\t1.9548155e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2063\t0.71588886\t8.325963e-08\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t938\t0.7111841\t7.5429443e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73366445\t5.4835597e-05\n",
      "\t\t2000\t0.7042743\t4.239919e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2704\t0.690862\t9.4903424e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7389234\t5.138044e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1615\t0.7168249\t3.3260378e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7078729\t8.099621e-05\n",
      "\t\t2000\t0.6783409\t3.312524e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2572\t0.66711617\t4.4673382e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.71658295\t3.892625e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1483\t0.703811\t9.3157183e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7435407\t0.00015901861\n",
      "\t\t2000\t0.7094678\t3.057986e-05\n",
      "\t\t3000\t0.69232154\t2.1436916e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3515\t0.6860168\t7.8196535e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72086746\t2.8111977e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1542\t0.7088373\t5.045277e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7227303\t4.791364e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1624\t0.70368874\t1.6940625e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.70681643\t5.9700888e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1715\t0.68903506\t8.650452e-08\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7372127\t3.4926565e-05\n",
      "\t\t2000\t0.715268\t2.874868e-05\n",
      "\t\t3000\t0.6909093\t2.6656673e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3797\t0.6801545\t1.7526799e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.715095\t8.7262e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1962\t0.68995\t5.183391e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7161116\t3.845251e-05\n",
      "\t\t2000\t0.68871564\t4.0760864e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2343\t0.68255085\t6.112845e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73027074\t6.8800866e-05\n",
      "\t\t2000\t0.69644344\t3.8169153e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2865\t0.6802793\t4.380893e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72494876\t3.9299193e-05\n",
      "\t\t2000\t0.69966817\t4.2337575e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2815\t0.683066\t3.490419e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7191655\t4.0112453e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1805\t0.6930467\t2.580113e-07\n",
      "Cross validation fold 7/10\n",
      "[0.367] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6802793145179749 m2 (neural network) error\n",
      "[-0.974] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.364] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7558015\t3.241157e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1981\t0.74655885\t7.9839175e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78196466\t7.431309e-05\n",
      "\t\t2000\t0.7539998\t8.616515e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2937\t0.75009286\t7.9462956e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7599982\t2.1410207e-05\n",
      "\t\t2000\t0.74903995\t1.27317935e-05\n",
      "\t\t3000\t0.73722255\t5.25524e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3479\t0.73546946\t9.72515e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8230822\t0.0001275814\n",
      "\t\t2000\t0.75467026\t6.5865854e-05\n",
      "\t\t3000\t0.7378355\t1.0824819e-05\n",
      "\t\t4000\t0.731379\t6.5196577e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4606\t0.7282168\t4.9110054e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7377952\t3.6595407e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1696\t0.7304163\t8.1603666e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74534726\t1.3354638e-05\n",
      "\t\t2000\t0.7350538\t9.8927385e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2416\t0.7329032\t4.8796034e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7380269\t1.1225814e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1596\t0.7344253\t4.057909e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76340646\t7.580722e-05\n",
      "\t\t2000\t0.7395611\t1.6843993e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2352\t0.73539865\t4.0525381e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74736285\t3.301677e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1890\t0.72670937\t4.921193e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7502423\t2.359526e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1838\t0.72828966\t9.002606e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7327451\t1.3828341e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1767\t0.72506547\t9.864696e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.725825\t4.450698e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1976\t0.70660275\t4.217693e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7500851\t4.894733e-05\n",
      "\t\t2000\t0.72933006\t1.5200656e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2334\t0.7265826\t7.3830756e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75029814\t1.7000144e-05\n",
      "\t\t2000\t0.7353744\t2.431545e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2576\t0.72706044\t6.558421e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7381884\t2.8744213e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1744\t0.7251273\t7.397893e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7372925\t2.829411e-05\n",
      "\t\t2000\t0.71567285\t2.4068717e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2890\t0.69870436\t9.383804e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7251226\t3.287868e-05\n",
      "\t\t2000\t0.70649827\t3.290275e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2706\t0.69771135\t2.5628634e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.744938\t1.400206e-05\n",
      "\t\t2000\t0.72953695\t2.8104707e-05\n",
      "\t\t3000\t0.70732087\t2.3678798e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3192\t0.7046246\t1.6918125e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73423445\t4.2779666e-05\n",
      "\t\t2000\t0.7107191\t2.9184259e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2294\t0.7056995\t2.533853e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72352254\t3.253951e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1814\t0.7008967\t9.3544526e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73642087\t2.565678e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1975\t0.72035\t1.6548798e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73441446\t5.5185355e-05\n",
      "\t\t2000\t0.7165257\t1.7967775e-05\n",
      "\t\t3000\t0.6993512\t2.2499815e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3483\t0.69405395\t0.0\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.723816\t3.4255507e-05\n",
      "\t\t2000\t0.70545954\t1.2588929e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2271\t0.70255214\t8.4840246e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72623193\t4.7354366e-05\n",
      "\t\t2000\t0.69936985\t1.4829141e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2160\t0.6964174\t9.4146196e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73456603\t5.6228706e-05\n",
      "\t\t2000\t0.7002289\t3.9750234e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2653\t0.68721575\t4.3366782e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72643054\t3.44604e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1769\t0.7087176\t5.8871444e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74505806\t2.895916e-05\n",
      "\t\t2000\t0.7240986\t2.502333e-05\n",
      "\t\t3000\t0.70722383\t2.3092118e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3477\t0.6995219\t5.9645345e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7314137\t6.567854e-05\n",
      "\t\t2000\t0.7010117\t3.111877e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2761\t0.6844515\t6.9667095e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7281297\t8.324462e-05\n",
      "\t\t2000\t0.6945666\t2.4456845e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2244\t0.69084615\t8.627774e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73554176\t4.4324195e-05\n",
      "\t\t2000\t0.7072617\t3.8680835e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2608\t0.6978673\t7.6868685e-07\n",
      "Cross validation fold 8/10\n",
      "[0.572] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]\n",
      " [6.]\n",
      " [9.]\n",
      " [8.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.6844515204429626 m2 (neural network) error\n",
      "[1.328] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.579] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76821524\t2.0327765e-05\n",
      "\t\t2000\t0.7575371\t6.373213e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2140\t0.7566589\t6.3018814e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7612722\t1.3231866e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1609\t0.7562863\t5.516856e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78002226\t3.2933363e-05\n",
      "\t\t2000\t0.7672717\t1.1652447e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2560\t0.76263326\t9.3787554e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78035927\t1.7796463e-05\n",
      "\t\t2000\t0.7668699\t1.305756e-05\n",
      "\t\t3000\t0.75827336\t8.567953e-06\n",
      "\t\t4000\t0.7509835\t5.9526233e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4207\t0.7498894\t7.948452e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7700851\t2.0278409e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1905\t0.7589279\t7.068421e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76205313\t3.6056215e-05\n",
      "\t\t2000\t0.7491969\t9.467319e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2084\t0.74865985\t2.388453e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7712578\t3.948974e-05\n",
      "\t\t2000\t0.7556046\t7.730511e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2031\t0.7554189\t7.8902707e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.80589646\t0.00019950609\n",
      "\t\t2000\t0.76573414\t1.5489893e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2795\t0.7583698\t8.64554e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7734105\t2.6587506e-05\n",
      "\t\t2000\t0.7551892\t9.786824e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2075\t0.7545038\t2.3699535e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77433723\t2.0474943e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1824\t0.76569074\t9.3413047e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7657646\t1.6111951e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1834\t0.756943\t2.3623166e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75762975\t3.6188052e-05\n",
      "\t\t2000\t0.74183834\t1.9523992e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2644\t0.734267\t4.0587872e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.763607\t1.943574e-05\n",
      "\t\t2000\t0.75117993\t2.285171e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2037\t0.750506\t7.1477405e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75419134\t2.4024906e-05\n",
      "\t\t2000\t0.7369002\t1.9573941e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2645\t0.72950935\t3.2682055e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7668879\t2.362716e-05\n",
      "\t\t2000\t0.74996406\t1.7564034e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2522\t0.7408162\t4.0229023e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7629909\t2.7575505e-05\n",
      "\t\t2000\t0.73643774\t2.3390083e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2155\t0.7345939\t8.925362e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73230946\t3.8985585e-05\n",
      "\t\t2000\t0.71375144\t2.6054118e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2017\t0.71352285\t8.353565e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74536574\t4.150114e-05\n",
      "\t\t2000\t0.72054374\t3.8877715e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2160\t0.7173803\t3.3234605e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7511869\t3.8640635e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1858\t0.7338388\t6.4978417e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7336511\t4.1595125e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1268\t0.72715795\t8.196933e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73840964\t4.487847e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1660\t0.7242975\t1.6458611e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7484895\t3.4241122e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1919\t0.7264509\t8.204909e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74303657\t2.919837e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1706\t0.73016465\t0.0\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7317588\t6.0272294e-05\n",
      "\t\t2000\t0.705721\t3.3106917e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2034\t0.7051363\t0.0\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74872357\t3.4150813e-05\n",
      "\t\t2000\t0.7266334\t2.4607942e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2458\t0.71854746\t8.295158e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7277177\t4.7257687e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1106\t0.7244863\t9.872582e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.72333235\t6.82249e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1865\t0.70240974\t5.9400196e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7460346\t4.0425377e-05\n",
      "\t\t2000\t0.71432984\t4.3888223e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2145\t0.71088576\t8.3845606e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7496672\t4.3727563e-05\n",
      "\t\t2000\t0.7141447\t3.6471993e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2705\t0.70235604\t9.335033e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7332812\t4.2510164e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1713\t0.7125259\t5.8556856e-07\n",
      "Cross validation fold 9/10\n",
      "[0.439] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]\n",
      " [6.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]] m2 (neural network) optimal h value\n",
      "0.7023560404777527 m2 (neural network) error\n",
      "[0.903] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.43] m4 (rlr_validate) test error\n",
      "10\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7759911\t1.8895147e-05\n",
      "\t\t2000\t0.7606763\t9.167736e-06\n",
      "\t\t3000\t0.75170016\t4.1232247e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3350\t0.7508327\t7.1446203e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77217615\t1.5051916e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1869\t0.7632144\t5.466777e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78010297\t2.3990948e-05\n",
      "\t\t2000\t0.77035725\t8.201442e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2055\t0.7700773\t7.7400856e-08\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78073776\t2.7330405e-05\n",
      "\t\t2000\t0.76619434\t1.2602326e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2994\t0.7521628\t5.5471065e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77207273\t2.2001748e-05\n",
      "\t\t2000\t0.7597775\t1.7023389e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2974\t0.74934185\t7.1588346e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77300346\t3.8860853e-05\n",
      "\t\t2000\t0.75764954\t9.833712e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2635\t0.7524602\t3.9606496e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76854\t6.9252426e-05\n",
      "\t\t2000\t0.75163203\t8.881555e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2668\t0.7481809\t9.55992e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7497724\t3.25927e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1235\t0.7459353\t3.9952974e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7521038\t3.0352041e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1725\t0.7410384\t4.0216992e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75069875\t2.1675427e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1670\t0.73546475\t8.104352e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7518876\t3.2104672e-05\n",
      "\t\t2000\t0.7326893\t2.3672435e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2274\t0.7300724\t8.16421e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7651488\t4.2063915e-05\n",
      "\t\t2000\t0.74711347\t1.7232163e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2458\t0.74080855\t4.0229438e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75591195\t2.199903e-05\n",
      "\t\t2000\t0.73722184\t6.338266e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2490\t0.7244613\t8.227444e-08\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7613375\t3.9691142e-05\n",
      "\t\t2000\t0.74175984\t1.7597593e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2476\t0.736802\t5.6627465e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77006876\t6.578713e-05\n",
      "\t\t2000\t0.74447197\t1.9615045e-05\n",
      "\t\t3000\t0.7341652\t1.5993577e-05\n",
      "\t\t4000\t0.72174\t1.4369522e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4026\t0.72152936\t8.260869e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74565154\t7.54542e-05\n",
      "\t\t2000\t0.71611476\t2.8299426e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2032\t0.715688\t1.6656604e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75548404\t4.544201e-05\n",
      "\t\t2000\t0.7351638\t2.5457432e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2381\t0.7295224\t5.719259e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7393889\t4.5705703e-05\n",
      "\t\t2000\t0.71456903\t2.5940899e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2199\t0.7119206\t5.860657e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74193895\t5.301916e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1906\t0.72038776\t7.4465646e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7485009\t3.4877612e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1642\t0.7391142\t5.645031e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7407874\t2.8080172e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1988\t0.7187879\t4.146193e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8225495\t0.0002143729\n",
      "\t\t2000\t0.7505824\t1.7073115e-05\n",
      "\t\t3000\t0.7368401\t1.7957753e-05\n",
      "\t\t4000\t0.72610605\t1.559649e-05\n",
      "\t\tFinal loss:\n",
      "\t\t4049\t0.7256353\t1.6428262e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.77639735\t0.00017009515\n",
      "\t\t2000\t0.7374262\t2.9986268e-05\n",
      "\t\t3000\t0.7167086\t2.1123313e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3997\t0.6980351\t9.392801e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74345404\t6.0045673e-05\n",
      "\t\t2000\t0.71672493\t3.301443e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2821\t0.6995732\t8.5201367e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7359209\t4.324854e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1884\t0.71469146\t5.003945e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7392997\t3.781081e-05\n",
      "\t\t2000\t0.7117248\t3.1320305e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2177\t0.70738643\t8.426038e-08\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73872536\t2.1865373e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1441\t0.72552\t1.6430873e-07\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.73016137\t4.9874758e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1312\t0.72093105\t4.133864e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7605159\t4.1536474e-05\n",
      "\t\t2000\t0.7329524\t2.732321e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2879\t0.71958107\t4.1416195e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7440869\t4.5256944e-05\n",
      "\t\t2000\t0.718469\t5.2264927e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2421\t0.7062937\t8.439081e-07\n",
      "Cross validation fold 10/10\n",
      "[0.357] m1 (regression) test error\n",
      "[[9.]\n",
      " [7.]\n",
      " [8.]\n",
      " [9.]\n",
      " [9.]\n",
      " [9.]\n",
      " [6.]\n",
      " [9.]\n",
      " [9.]\n",
      " [7.]] m2 (neural network) optimal h value\n",
      "0.7062937021255493 m2 (neural network) error\n",
      "[-1.753] m3 (baseline) test error\n",
      "10.0 m4 (rlr_validate) optimal lambda value\n",
      "[0.356] m4 (rlr_validate) test error\n"
     ]
    }
   ],
   "source": [
    "print(\"foo\")\n",
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot, train_neural_net, rlr_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load data from matlab file\n",
    "X = X_without_age\n",
    "y = X_only_age\n",
    "# attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "# N, M = X.shape\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10 #Folds of cross validation\n",
    "S = 3  #Number of different models to crossvalidate\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "\n",
    "\n",
    "#ANN model:\n",
    "# n_hidden_units = 3 \n",
    "\n",
    "h_values = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "\n",
    "# m2 = lambda: torch.nn.Sequential(\n",
    "#                 torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "#                 # 1st transfer function, either Tanh or ReLU:\n",
    "#                 torch.nn.ReLU(),                            #torch.nn.ReLU(),\n",
    "#                 torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "#                 torch.nn.ReLU() # final tranfer function\n",
    "#                 )\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "max_iter = 10000\n",
    "# print('Training model of type:\\n{}\\n'.format(str(m2())))\n",
    "\n",
    "# Initialize variables\n",
    "Features = np.zeros((M,K))\n",
    "m1_Error_train = np.empty((K,1))\n",
    "m1_Error_test = np.empty((K,1))\n",
    "\n",
    "m3_Error_train = np.empty((K,1))\n",
    "m3_Error_test = np.empty((K,1))\n",
    "\n",
    "m4_w_rlr = np.empty((M,K))\n",
    "m4_Error_train_rlr = np.empty((K,1))\n",
    "m4_Error_test_rlr = np.empty((K,1))\n",
    "m4_optimal_lambda = np.empty((K,1))\n",
    "\n",
    "m2_errors = np.empty((K,1))\n",
    "m2_optimal_h = np.empty((K,1))\n",
    "\n",
    "Error_train_fs = np.empty((K,1))\n",
    "Error_test_fs = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X):\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Extract training and test set for current CV fold, \n",
    "    # and convert them to PyTorch tensors\n",
    "    X_train_torch = torch.Tensor(X[train_index,:])\n",
    "    y_train_torch = torch.Tensor(y[train_index]).unsqueeze(1)\n",
    "    X_test_torch = torch.Tensor(X[test_index,:])\n",
    "    y_test_torch = torch.Tensor(y[test_index]).unsqueeze(1)\n",
    "    \n",
    "    # Go to the file 'toolbox_02450.py' in the Tools sub-folder of the toolbox\n",
    "    # and see how the network is trained (search for 'def train_neural_net',\n",
    "    # which is the place the function below is defined)\n",
    "\n",
    "    #Model 2: (ANN)\n",
    "\n",
    "    temp_m2_errors = np.empty((len(h_values),1))\n",
    "    print(len(h_values))\n",
    "    nr = 0\n",
    "    for j in h_values:\n",
    "\n",
    "\n",
    "        m2 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, j), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    torch.nn.Tanh(),                            #torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(j, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.ReLU() # final tranfer function\n",
    "                    )\n",
    "\n",
    "        # print('Training model of type:\\n{}\\n'.format(str(m2())))\n",
    "\n",
    "        m2_net, m2_final_loss, m2_learning_curve = train_neural_net(m2,\n",
    "                                                        loss_fn,\n",
    "                                                        X=X_train_torch,\n",
    "                                                        y=y_train_torch,\n",
    "                                                        n_replicates=3,\n",
    "                                                        max_iter=max_iter)\n",
    "\n",
    "        temp_m2_errors[nr] = m2_final_loss\n",
    "\n",
    "        nr += 1\n",
    "    \n",
    "    minimum_m2_loss_index = np.argmin(temp_m2_errors)\n",
    "    minimum_m2_loss = temp_m2_errors[minimum_m2_loss_index]\n",
    "\n",
    "    m2_errors[k] = minimum_m2_loss\n",
    "    m2_optimal_h[k] = minimum_m2_loss_index\n",
    "    \n",
    "    #model 3: (Baseline)\n",
    "    m3_mean_y = np.mean(y_train)\n",
    "    m3_Error_train[k] = y_train[k] - m3_mean_y \n",
    "    m3_Error_test[k] = y_test[k] - m3_mean_y\n",
    "\n",
    "    m4_internal_cross_validation = 10  \n",
    "    # Values of lambda\n",
    "    m4_lambdas = np.power(10.,range(-5,9))\n",
    "    m4_opt_val_err, m4_opt_lambda, m4_mean_w_vs_lambda, m4_train_err_vs_lambda, m4_test_err_vs_lambda = rlr_validate(X_train, y_train, m4_lambdas, m4_internal_cross_validation)\n",
    "\n",
    "    m4_Xty = X_train.T @ y_train\n",
    "    m4_XtX = X_train.T @ X_train\n",
    "\n",
    "    m4_lambdaI = m4_opt_lambda * np.eye(M)\n",
    "    m4_lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    m4_w_rlr[:,k] = np.linalg.solve(m4_XtX+m4_lambdaI,m4_Xty).squeeze()\n",
    "\n",
    "    m4_optimal_lambda[k] = m4_opt_lambda\n",
    "    m4_Error_train_rlr[k] = np.square(y_train-X_train @ m4_w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    m4_Error_test_rlr[k] = np.square(y_test-X_test @ m4_w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    print('Cross validation fold {0}/{1}'.format(k+1,K))\n",
    "    # print('Train indices: {0}'.format(train_index))\n",
    "    # print('Test indices: {0}'.format(test_index))\n",
    "\n",
    "    print(f\"{m2_optimal_h} m2 (neural network) optimal h value\")\n",
    "    print(f'{m2_final_loss} m2 (neural network) error')\n",
    "\n",
    "    #print(f'{m3_Error_train[k]} m3 (baseline) training error')\n",
    "    print(f'{m3_Error_test[k]} m3 (baseline) test error')\n",
    "\n",
    "    #print(f'{m4_Error_train_rlr[k]} m4 (rlr_validate) training error')\n",
    "    print(f\"{m4_opt_lambda} m4 (rlr_validate) optimal lambda value\")\n",
    "    print(f'{m4_Error_test_rlr[k]} m4 (rlr_validate) test error')\n",
    "\n",
    "\n",
    "    k+=1\n",
    "\n",
    "    ### Skal man bruge rlr_validate eller skal man lave linear regression \n",
    "    ### og så selv sætte lamdas ind så man kan lave et plot over lambda \n",
    "    ### værdierne og deres tilhørende test og training error? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: \n",
    "\n",
    "Produce a table akin to Table 1 using two-level cross-validation (algorithm 6\n",
    "in the lecture notes). The table shows, for each of the K 1 = 10 folds i, the\n",
    "optimal value of the number of hidden units and regularization strength (h ∗ i\n",
    "and λ ∗ i respectively) as found after each inner loop, as well as the estimated\n",
    "generalization errors E i test by evaluating on D i test . It also includes the baseline\n",
    "test error, also evaluated on D i test . Importantly, you must re-use the train/test\n",
    "splits D i par , D i test for all three methods to allow statistical comparison (see next\n",
    "section).\n",
    "Note the error measure we use is the squared loss per observation, i.e. we divide\n",
    "by the number of observation in the test dataset:\n",
    "\n",
    "$$\n",
    "\n",
    "E = \\frac{1}{N^{test}}\\,\\sum_{i=1}^{N^{test}}(y_i - ŷ_i)^2\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Include a table similar to Table 1 in your report and briefly discuss what it tells\n",
    "you at a glance. Do you find the same value of λ ∗ as in the previous section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outer fold  ANN_h  ANN_E  LR_lamda  LR_E  BaseLine_E\n",
      "0           0   9.00   0.70     10.00  0.40        1.06\n",
      "1           1   7.00   0.67     10.00  0.61       -1.91\n",
      "2           2   8.00   0.69     10.00  0.36       -0.19\n",
      "3           3   9.00   0.65      1.00  0.89        0.07\n",
      "4           4   9.00   0.66     10.00  0.32       -0.70\n",
      "5           5   9.00   0.68     10.00  0.43       -0.24\n",
      "6           6   6.00   0.67     10.00  0.36       -0.97\n",
      "7           7   9.00   0.68     10.00  0.58        1.33\n",
      "8           8   9.00   0.70     10.00  0.43        0.90\n",
      "9           9   7.00   0.70     10.00  0.36       -1.75\n",
      "\n",
      "\n",
      "ANN = artificial neuralnet (h = hiddenlayer, E = error)\n",
      "LR = linear regression (lamda = optimal lambda number, E = error)\n",
      "Baseline_E is just the error of the test set mean\n"
     ]
    }
   ],
   "source": [
    "# Turn data into pandas dataframe so that i can display it:\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "data = {\"Outer fold\":[i for i in range(K)],\n",
    "        \"ANN_h\":m2_optimal_h.squeeze(),\n",
    "        \"ANN_E\":m2_errors.squeeze(),\n",
    "        \"LR_lamda\":m4_optimal_lambda.squeeze(),\n",
    "        \"LR_E\":m4_Error_test_rlr.squeeze(),\n",
    "        \"BaseLine_E\":m3_Error_test.squeeze()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "print(\"ANN = artificial neuralnet (h = hiddenlayer, E = error)\")\n",
    "print(\"LR = linear regression (lamda = optimal lambda number, E = error)\")\n",
    "print(\"Baseline_E is just the error of the test set mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: \n",
    "\n",
    "Statistically evaluate if there is a significant performance difference between the\n",
    "fitted ANN, linear regression model and baseline using the methods described\n",
    "in chapter 11. These comparisons will be made pairwise (ANN vs. linear\n",
    "regression; ANN vs. baseline; linear regression vs. baseline). We will allow\n",
    "some freedom in what test to choose. Therefore, choose either:\n",
    "setup I (section 11.3): Use the paired t-test described in Box 11.3.4\n",
    "setup II (section 11.4): Use the method described in Box 11.4.1)\n",
    "Include p-values and confidence intervals for the three pairwise tests in your\n",
    "report and conclude on the results: Is one model better than the other? Are\n",
    "the two models better than the baseline? Are some of the models identical?\n",
    "What recommendations would you make based on what you’ve learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part classification: \n",
    "\n",
    "In this part of the report you are to solve a relevant classification\n",
    "problem for your data and statistically evaluate your result. The tasks will closely\n",
    "mirror what you just did in the last section. The three methods we will compare is a\n",
    "baseline, logistic regression, and one of the other four methods from below (referred\n",
    "to as method 2 ).\n",
    "Logistic regression for classification. Once more, we can use a regularization pa-\n",
    "rameter λ ≥ 0 to control complexity\n",
    "ANN Artificial neural networks for classification. Same complexity-controlling pa-\n",
    "rameter as in the previous exercise\n",
    "CT Classification trees. Same complexity-controlling parameter as for regression\n",
    "trees\n",
    "KNN k-nearest neighbor classification, complexity controlling parameter k = 1, 2 . . .\n",
    "NB Naı̈ve Bayes. As complexity-controlling parameter, we suggest the term b ≥ 0\n",
    "+\n",
    "from section 11.2.1 of the lecture notes to estimate 6 $ p(x = 1) = \\frac{n^+ + b}{n^+ + n^- +2b} $\n",
    "\n",
    "\n",
    "1:\n",
    "\n",
    "Explain which classification problem you have chosen to solve. Is it a multi-\n",
    "class or binary classification problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:\n",
    "\n",
    "We will compare logistic regression 7 , method 2 and a baseline. For logistic\n",
    "regression, we will once more use λ as a complexity-controlling parameter, and\n",
    "for method 2 a relevant complexity controlling parameter and range of values.\n",
    "We recommend this choice is made based on a trial run, which you do not need\n",
    "to report. Describe which parameter you have chosen and the possible values\n",
    "of the parameters you will examine.\n",
    "The baseline will be a model which compute the largest class on the training\n",
    "data, and predict everything in the test-data as belonging to that class (corre-\n",
    "sponding to the optimal prediction by a logistic regression model with a bias\n",
    "term and no features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3:\n",
    "\n",
    "Again use two-level cross-validation to create a table similar to Table 2, but\n",
    "now comparing the logistic regression, method 2, and baseline. The table should\n",
    "once more include the selected parameters, and as an error measure we will use\n",
    "the error rate:\n",
    "$ E = \\frac{Number of misclassified observations}{N^{test}} $\n",
    "Once more, make sure to re-use the outer validation splits to admit statistical\n",
    "evaluation. Briefly discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4:\n",
    "\n",
    "Perform a statistical evaluation of your three models similar to the previous\n",
    "section. That is, compare the three models pairwise. We will once more allow\n",
    "some freedom in what test to choose. Therefore, choose either:\n",
    "\n",
    "setup I (section 11.3): Use McNemera’s test described in Box 11.3.2)\n",
    "\n",
    "setup II (section 11.4): Use the method described in Box 11.4.1)\n",
    "\n",
    "Include p-values and confidence intervals for the three pairwise tests in your\n",
    "report and conclude on the results: Is one model better than the other? Are\n",
    "the two models better than the baseline? Are some of the models identical?\n",
    "What recommendations would you make based on what you’ve learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5:\n",
    "\n",
    "Train a logistic regression model using a suitable value of λ (see previous ex-\n",
    "ercise). Explain how the logistic regression model make a prediction. Are the\n",
    "same features deemed relevant as for the regression part of the report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
