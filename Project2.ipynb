{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression part a:\n",
    "In this section, you are to solve a relevant regression problem\n",
    "for your data and statistically evaluate the result. We will begin by examining the\n",
    "most elementary model, namely linear regression.\n",
    "\n",
    "1:\n",
    "\n",
    "Explain what variable is predicted based on which other variables and what\n",
    "you hope to accomplish by the regression. Mention your feature transformation\n",
    "choices such as one-of-K coding. Since we will use regularization momentarily,\n",
    "apply a feature transformation to your data matrix X such that each column\n",
    "has mean 0 and standard deviation 1 3 .\n",
    "\n",
    "Since out desired output variable is a boolean we cant do regression on it so we have therefore picked age instead to try and see if linear regression can estimate the subjects age based on the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chd', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read data\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import sys\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 7.5})\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as lm\n",
    "from toolbox_02450 import rlr_validate\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\"\"\"\n",
    "Explanation of variables in dataset SAHD.xls (african heart disease)\n",
    "chd: Coronary heart disease (bool (Actually its a string but its only used in the dictionary and as whether or not the subject has the disease)) \n",
    "sbp: Systolic blood pressure (int)\n",
    "tobacco: tobacco in kg (float)\n",
    "ldl: ? (float)\n",
    "adiposity: ? (float)\n",
    "famhist: Family history of CHD (bool (Present, Absent)) (string)\n",
    "typea: ? (int)\n",
    "obesity: ? (float)\n",
    "alcohol: alcohol consumption in liters (float)\n",
    "age: age (int)\n",
    "\"\"\"\n",
    "\n",
    "# Load xls sheet with data\n",
    "doc = xlrd.open_workbook('Data/SAHD2.xls').sheet_by_index(0)\n",
    "\n",
    "# Extract attribute names (1st row, column 4 to 12)\n",
    "attributeNames = doc.row_values(0, 0, 10)\n",
    "print(attributeNames)\n",
    "\n",
    "# Extract class names to python list,\n",
    "# then encode with integers (dict)\n",
    "classLabels = doc.col_values(0, 1, 463)\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = dict(zip(classNames, range(2)))\n",
    "\n",
    "# Extract vector y, convert to NumPy array\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Preallocate memory, then extract excel data to matrix X\n",
    "X = np.empty((462, 10))\n",
    "for i, col_id in enumerate(range(0,10)):\n",
    "    X[:, i] = np.asarray(doc.col_values(col_id, 1, 463))\n",
    "# Compute values of N, M and C.\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)\n",
    "\n",
    "#print(X)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "\n",
    "\n",
    "Y2 = X - np.ones((N, 1))*X.mean(0)\n",
    "X = (Y2*(1/np.std(Y2,0)))\n",
    "\n",
    "\n",
    "X_only_age = X[:,[9]].squeeze()\n",
    "X_without_age = np.delete(X,9,axis=1)\n",
    "\n",
    "# print(Y2_without_age)\n",
    "# print(Y2)\n",
    "eps_mean, eps_std = 0, 0.1\n",
    "eps = np.array(eps_std*np.random.randn(N) + eps_mean).reshape(-1,1)\n",
    "w0 = -0.5\n",
    "w1 = 0.01\n",
    "y = w0 + w1*X[:,[9]] + eps\n",
    "y_true = y - eps\n",
    "\n",
    "\n",
    "# Fit ordinary least squares regression model\n",
    "model = lm.LinearRegression(fit_intercept=True)\n",
    "model = model.fit(X,y)\n",
    "# Compute model output:\n",
    "y_est = model.predict(X)\n",
    "# Or equivalently:\n",
    "#y_est = model.intercept_ + X @ model.coef_\n",
    "\n",
    "# Plot original data and the model output\n",
    "f = plt.figure()\n",
    "\n",
    "plt.plot(y_est,y_true,'.')\n",
    "plt.xlim([-0.55, -0.45])\n",
    "plt.ylim([-0.55, -0.45])\n",
    "\n",
    "plt.xlabel('y_est'); plt.ylabel('y_true')\n",
    "plt.legend(['Training data', 'Data generator', 'Regression fit (model)'])\n",
    "\n",
    "\n",
    "### Implementer ting fra ex8. \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:\n",
    "\n",
    "Introduce a regularization parameter λ as discussed in chapter 14 of the lecture\n",
    "notes, and estimate the generalization error for different values of λ. Specifi-\n",
    "cally, choose a reasonable range of values of λ (ideally one where the general-\n",
    "ization error first drop and then increases), and for each value use K = 10 fold\n",
    "cross-validation (algorithm 5) to estimate the generalization error.\n",
    "Include a figure of the estimated generalization error as a function of λ in the\n",
    "report and briefly discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 9\n",
      "N: 415 M: 9\n",
      "N: 415 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n",
      "N: 416 M: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression without feature selection:\n",
      "- Training error: 0.44290721504316766\n",
      "- Test error:     0.4581653624594194\n",
      "- R^2 train:     0.5569375699863099\n",
      "- R^2 test:     0.5283449502365718\n",
      "\n",
      "Regularized linear regression:\n",
      "- Training error: 0.44343995687725163\n",
      "- Test error:     0.4587246551958195\n",
      "- R^2 train:     0.5564046414550912\n",
      "- R^2 test:     0.5277691903362531\n",
      "\n",
      "Weights in last fold:\n",
      "            chd            0.12\n",
      "            sbp            0.15\n",
      "        tobacco            0.23\n",
      "            ldl            0.01\n",
      "      adiposity            0.68\n",
      "        famhist             0.1\n",
      "          typea           -0.06\n",
      "        obesity           -0.27\n",
      "        alcohol           -0.01\n",
      "Ran Exercise 8.1.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# exercise 8.1.1\n",
    "\n",
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import rlr_validate\n",
    "\n",
    "attributeNames = ['chd', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    "\n",
    "# X_only_age = X[:,[8]].squeeze()\n",
    "\n",
    "N, M = X_without_age.shape\n",
    "print(N,M)\n",
    "\n",
    "# Add offset attribute\n",
    "#X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "#attributeNames = [u'Offset']+attributeNames\n",
    "#M = M+1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "w_noreg = np.empty((M,K))\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X_without_age, X_only_age):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X_without_age[train_index]\n",
    "    y_train = X_only_age[train_index]\n",
    "    X_test = X_without_age[test_index]\n",
    "    y_test = X_only_age[test_index]\n",
    "\n",
    "    \n",
    "    N, M = X_train.shape\n",
    "    print(f'N: {N} M: {M}')\n",
    "\n",
    "    internal_cross_validation = 10    \n",
    "    \n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    \n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    \n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = np.square(y_train-y_train.mean()).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_nofeatures[k] = np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = np.square(y_train-X_train @ w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_rlr[k] = np.square(y_test-X_test @ w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:,k] = np.linalg.solve(XtX,Xty).squeeze()\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    #m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    # Display the results for the last cross-validation fold\n",
    "    if k == K-1:\n",
    "        figure(k, figsize=(12,8))\n",
    "        subplot(1,2,1)\n",
    "        semilogx(lambdas,mean_w_vs_lambda.T[:,1:],'.-') # Don't plot the bias term\n",
    "        xlabel('Regularization factor')\n",
    "        ylabel('Mean Coefficient Values')\n",
    "        grid()\n",
    "        # You can choose to display the legend, but it's omitted for a cleaner \n",
    "        # plot, since there are many attributes\n",
    "        #legend(attributeNames[1:], loc='best')\n",
    "        \n",
    "        subplot(1,2,2)\n",
    "        title('Optimal lambda: 1e{0}'.format(np.log10(opt_lambda)))\n",
    "        loglog(lambdas,train_err_vs_lambda.T,'b.-',lambdas,test_err_vs_lambda.T,'r.-')\n",
    "        xlabel('Regularization factor')\n",
    "        ylabel('Squared error (crossvalidation)')\n",
    "        legend(['Train error','Validation error'])\n",
    "        grid()\n",
    "    \n",
    "    # To inspect the used indices, use these print statements\n",
    "    #print('Cross validation fold {0}/{1}:'.format(k+1,K))\n",
    "    #print('Train indices: {0}'.format(train_index))\n",
    "    #print('Test indices: {0}\\n'.format(test_index))\n",
    "\n",
    "    k+=1\n",
    "\n",
    "show()\n",
    "# Display results\n",
    "print('Linear regression without feature selection:')\n",
    "print('- Training error: {0}'.format(Error_train.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test.sum())/Error_test_nofeatures.sum()))\n",
    "print('Regularized linear regression:')\n",
    "print('- Training error: {0}'.format(Error_train_rlr.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test_rlr.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train_rlr.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_rlr.sum())/Error_test_nofeatures.sum()))\n",
    "\n",
    "print('Weights in last fold:')\n",
    "for m in range(M):\n",
    "    print('{:>15} {:>15}'.format(attributeNames[m], np.round(w_rlr[m,-1],2)))\n",
    "\n",
    "print('Ran Exercise 8.1.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3: Jeg forstår ikke spørgsmålet..\n",
    "3:\n",
    "\n",
    "Explain how a new data observation is predicted according to the linear model\n",
    "with the lowest generalization error as estimated in the previous question. I.e.,\n",
    "what are the effects of the selected attributes in terms of determining the\n",
    "predicted class. Does the result make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression part b:\n",
    "In this section, we will compare three models: the regularized\n",
    "linear regression model from the previous section, an artificial neural network (ANN)\n",
    "and a baseline. We are interested in two questions: Is one model better than the\n",
    "other? Is either model better than a trivial baseline?. We will attempt to answer\n",
    "these questions with two-level cross-validation.\n",
    "\n",
    "1:\n",
    "\n",
    "Implement two-level cross-validation (see algorithm 6 of the lecture notes). We\n",
    "will use 2-level cross-validation to compare the models with K 1 = K 2 = 10 \n",
    "folds. As a baseline model, we will apply a linear regression model with no\n",
    "features, i.e. it computes the mean of y on the training data, and use this value\n",
    "to predict y on the test data.\n",
    "Make sure you can fit an ANN model to the data. As complexity-controlling\n",
    "parameter for the ANN, we will use the number of hidden units 5 h. Based on\n",
    "a few test-runs, select a reasonable range of values for h (which should include\n",
    "h = 1), and describe the range of values you will use for h and λ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlr_validate_new(X,y,lambdas):\n",
    "    ''' Validate regularized linear regression model using 'cvf'-fold cross validation.\n",
    "        Find the optimal lambda (minimizing validation error) from 'lambdas' list.\n",
    "        The loss function computed as mean squared error on validation set (MSE).\n",
    "        Function returns: MSE averaged over 'cvf' folds, optimal value of lambda,\n",
    "        average weight values for all lambdas, MSE train&validation errors for all lambdas.\n",
    "        The cross validation splits are standardized based on the mean and standard\n",
    "        deviation of the training set when estimating the regularization strength.\n",
    "        \n",
    "        Parameters:\n",
    "        X       training data set\n",
    "        y       vector of values\n",
    "        lambdas vector of lambda values to be validated\n",
    "        cvf     number of crossvalidation folds     \n",
    "        \n",
    "        Returns:\n",
    "        opt_val_err         validation error for optimum lambda\n",
    "        opt_lambda          value of optimal lambda\n",
    "        mean_w_vs_lambda    weights as function of lambda (matrix)\n",
    "        train_err_vs_lambda train error as function of lambda (vector)\n",
    "        test_err_vs_lambda  test error as function of lambda (vector)\n",
    "    '''\n",
    "    # CV = model_selection.KFold(cvf, shuffle=True)\n",
    "    cvf  = 1\n",
    "    M = X.shape[1]\n",
    "    w = np.empty((M,cvf,len(lambdas)))\n",
    "    train_error = np.empty((cvf,len(lambdas)))\n",
    "    test_error = np.empty((cvf,len(lambdas)))\n",
    "    f = 0\n",
    "    y = y.squeeze()\n",
    "    for i in range(1):\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        X_test = X\n",
    "        y_test = y\n",
    "        \n",
    "        # Standardize the training and set set based on training set moments\n",
    "        mu = np.mean(X_train[:, 1:], 0)\n",
    "        sigma = np.std(X_train[:, 1:], 0)\n",
    "        \n",
    "        X_train[:, 1:] = (X_train[:, 1:] - mu) / sigma\n",
    "        X_test[:, 1:] = (X_test[:, 1:] - mu) / sigma\n",
    "        \n",
    "        # precompute terms\n",
    "        Xty = X_train.T @ y_train\n",
    "        XtX = X_train.T @ X_train\n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "            lambdaI = lambdas[l] * np.eye(M)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,f,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            # Evaluate training and test performance\n",
    "            train_error[f,l] = np.power(y_train-X_train @ w[:,f,l].T,2).mean(axis=0)\n",
    "            test_error[f,l] = np.power(y_test-X_test @ w[:,f,l].T,2).mean(axis=0)\n",
    "        \n",
    "        f=f+1\n",
    "\n",
    "    opt_val_err = np.min(np.mean(test_error,axis=0))\n",
    "    opt_lambda = lambdas[np.argmin(np.mean(test_error,axis=1))]\n",
    "    train_err_vs_lambda = np.mean(train_error,axis=0)\n",
    "    test_err_vs_lambda = np.mean(test_error,axis=0)\n",
    "    mean_w_vs_lambda = np.squeeze(np.mean(w,axis=1))\n",
    "    \n",
    "    return opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation fold 1/10\n",
      "7 m2 (neural network) optimal h value\n",
      "[0.70981884] m2 (neural network) error\n",
      "[1.05295112] m3 (baseline) test error\n",
      "1e-90 m4 (rlr_validate) optimal lambda value\n",
      "[0.32605195] m4 (rlr_validate) test error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5408/498052988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# print('Training model of type:\\n{}\\n'.format(str(m2())))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         m2_net, m2_final_loss, m2_learning_curve = train_neural_net(m2,\n\u001b[0m\u001b[1;32m     97\u001b[0m                                                         \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                                         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_torch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Machine_learning/toolbox_02450/__init__.py\u001b[0m in \u001b[0;36mtrain_neural_net\u001b[0;34m(model, loss_fn, X, y, n_replicates, max_iter, tolerance)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;31m#print(print_str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# do backpropagation of loss and optimize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot, train_neural_net, rlr_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load data from matlab file\n",
    "\n",
    "\n",
    "# attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "N, M = X_without_age.shape\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10 #Folds of cross validation\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "\n",
    "\n",
    "#ANN model:\n",
    "# n_hidden_units = 3 \n",
    "\n",
    "h_values = [1, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "\n",
    "# m2 = lambda: torch.nn.Sequential(\n",
    "#                 torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "#                 # 1st transfer function, either Tanh or ReLU:\n",
    "#                 torch.nn.ReLU(),                            #torch.nn.ReLU(),\n",
    "#                 torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "#                 torch.nn.ReLU() # final tranfer function\n",
    "#                 )\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "max_iter = 10000\n",
    "# print('Training model of type:\\n{}\\n'.format(str(m2())))\n",
    "\n",
    "# Initialize variables\n",
    "Features = np.zeros((M,K))\n",
    "m1_Error_train = np.empty((K,1))\n",
    "m1_Error_test = np.empty((K,1))\n",
    "\n",
    "m3_Error_train = np.empty((K,1))\n",
    "m3_Error_test = np.empty((K,1))\n",
    "\n",
    "m4_w_rlr = np.empty((M,K))\n",
    "m4_Error_train_rlr = np.empty((K,1))\n",
    "m4_Error_test_rlr = np.empty((K,1))\n",
    "m4_optimal_lambda = np.empty((K,1))\n",
    "\n",
    "m2_errors = np.empty((K,1))\n",
    "m2_optimal_h = np.empty((K,1))\n",
    "\n",
    "Error_train_fs = np.empty((K,1))\n",
    "Error_test_fs = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X_without_age):\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X_without_age[train_index,:]\n",
    "    y_train = X_only_age[train_index]\n",
    "    X_test = X_without_age[test_index,:]\n",
    "    y_test = X_only_age[test_index]\n",
    "\n",
    "    # Extract training and test set for current CV fold, \n",
    "    # and convert them to PyTorch tensors\n",
    "    X_train_torch = torch.Tensor(X_without_age[train_index,:])\n",
    "    y_train_torch = torch.Tensor(X_only_age[train_index]).unsqueeze(1)\n",
    "    X_test_torch = torch.Tensor(X_without_age[test_index,:])\n",
    "    y_test_torch = torch.Tensor(X_only_age[test_index]).unsqueeze(1)\n",
    "    \n",
    "    # Go to the file 'toolbox_02450.py' in the Tools sub-folder of the toolbox\n",
    "    # and see how the network is trained (search for 'def train_neural_net',\n",
    "    # which is the place the function below is defined)\n",
    "\n",
    "    #Model 2: (ANN)\n",
    "\n",
    "    temp_m2_errors = np.empty((len(h_values),1))\n",
    "    nr = 0\n",
    "    for j in h_values:\n",
    "\n",
    "\n",
    "        m2 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, j), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    torch.nn.ReLU(),                            #torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(j, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.ReLU() # final tranfer function\n",
    "                    )\n",
    "\n",
    "        # print('Training model of type:\\n{}\\n'.format(str(m2())))\n",
    "\n",
    "        m2_net, m2_final_loss, m2_learning_curve = train_neural_net(m2,\n",
    "                                                        loss_fn,\n",
    "                                                        X=X_train_torch,\n",
    "                                                        y=y_train_torch,\n",
    "                                                        n_replicates=1,\n",
    "                                                        max_iter=max_iter)\n",
    "\n",
    "        temp_m2_errors[nr] = m2_final_loss\n",
    "\n",
    "        nr += 1\n",
    "    \n",
    "    minimum_m2_loss_index = np.argmin(temp_m2_errors)\n",
    "    minimum_m2_loss = temp_m2_errors[minimum_m2_loss_index]\n",
    "\n",
    "    m2_errors[k] = minimum_m2_loss\n",
    "    m2_optimal_h[k] = minimum_m2_loss_index+1\n",
    "    \n",
    "    #model 3: (Baseline)\n",
    "    m3_mean_y = np.mean(y_train)\n",
    "    m3_Error_train[k] = y_train[k] - m3_mean_y \n",
    "    m3_Error_test[k] = 1/y_test.shape[0]*(sum((y_test-m3_mean_y)**2)) #y_test[k] - m3_mean_y\n",
    "\n",
    "\n",
    "\n",
    "    # Values of lambda\n",
    "    m4_lambdas = np.power(10.,range(-5,9))\n",
    "    m4_opt_val_err, m4_opt_lambda, m4_mean_w_vs_lambda, m4_train_err_vs_lambda, m4_test_err_vs_lambda = rlr_validate_new(X_train, y_train, m4_lambdas)\n",
    "\n",
    "    m4_Xty = X_train.T @ y_train\n",
    "    m4_XtX = X_train.T @ X_train\n",
    "\n",
    "    m4_lambdaI = m4_opt_lambda * np.eye(M)\n",
    "    m4_lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    m4_w_rlr[:,k] = np.linalg.solve(m4_XtX+m4_lambdaI,m4_Xty).squeeze()\n",
    "\n",
    "    m4_optimal_lambda[k] = m4_opt_lambda\n",
    "    m4_Error_train_rlr[k] = np.square(y_train-X_train @ m4_w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    m4_Error_test_rlr[k] = np.square(y_test-X_test @ m4_w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    print('Cross validation fold {0}/{1}'.format(k+1,K))\n",
    "    # print('Train indices: {0}'.format(train_index))\n",
    "    # print('Test indices: {0}'.format(test_index))\n",
    "\n",
    "    print(f\"{minimum_m2_loss_index} m2 (neural network) optimal h value\")\n",
    "    print(f'{minimum_m2_loss} m2 (neural network) error')\n",
    "\n",
    "    #print(f'{m3_Error_train[k]} m3 (baseline) training error')\n",
    "    print(f'{m3_Error_test[k]} m3 (baseline) test error')\n",
    "\n",
    "    #print(f'{m4_Error_train_rlr[k]} m4 (rlr_validate) training error')\n",
    "    print(f\"{m4_opt_lambda} m4 (rlr_validate) optimal lambda value\")\n",
    "    print(f'{m4_Error_test_rlr[k]} m4 (rlr_validate) test error')\n",
    "\n",
    "    k+=1\n",
    "\n",
    "    ### Skal man bruge rlr_validate eller skal man lave linear regression \n",
    "    ### og så selv sætte lamdas ind så man kan lave et plot over lambda \n",
    "    ### værdierne og deres tilhørende test og training error? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: \n",
    "\n",
    "Produce a table akin to Table 1 using two-level cross-validation (algorithm 6\n",
    "in the lecture notes). The table shows, for each of the K 1 = 10 folds i, the\n",
    "optimal value of the number of hidden units and regularization strength (h ∗ i\n",
    "and λ ∗ i respectively) as found after each inner loop, as well as the estimated\n",
    "generalization errors E i test by evaluating on D i test . It also includes the baseline\n",
    "test error, also evaluated on D i test . Importantly, you must re-use the train/test\n",
    "splits D i par , D i test for all three methods to allow statistical comparison (see next\n",
    "section).\n",
    "Note the error measure we use is the squared loss per observation, i.e. we divide\n",
    "by the number of observation in the test dataset:\n",
    "\n",
    "$$\n",
    "\n",
    "E = \\frac{1}{N^{test}}\\,\\sum_{i=1}^{N^{test}}(y_i - ŷ_i)^2\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Include a table similar to Table 1 in your report and briefly discuss what it tells\n",
    "you at a glance. Do you find the same value of λ ∗ as in the previous section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outer fold  ANN_h  ANN_E  LR_lamda  LR_E  BaseLine_E\n",
      "0           0      9   0.68     1e-05   0.4           1\n",
      "1           1      9   0.71     1e-05  0.46        0.88\n",
      "2           2      9    0.7     1e-05  0.46         1.1\n",
      "3           3     10   0.69     1e-05  0.37           1\n",
      "4           4      9   0.69     1e-05  0.64         1.2\n",
      "5           5     10    0.7     1e-05  0.35        0.94\n",
      "6           6     10   0.72     1e-05  0.39         0.8\n",
      "7           7     10   0.69     1e-05  0.64         1.1\n",
      "8           8      9   0.69     1e-05  0.41        0.78\n",
      "9           9      9   0.67     1e-05  0.52         1.2\n",
      "\n",
      "\n",
      "ANN = artificial neuralnet (h = hiddenlayer, E = error)\n",
      "LR = linear regression (lamda = optimal lambda number, E = error)\n",
      "Baseline_E is just the error of the test set mean\n",
      "Outer fold     4.5\n",
      "ANN_h          9.4\n",
      "ANN_E         0.69\n",
      "LR_lamda     1e-05\n",
      "LR_E          0.46\n",
      "BaseLine_E       1\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Turn data into pandas dataframe so that i can display it:\n",
    "# pd.options.display.float_format = '{:,.8f}'.format\n",
    "pd.set_option('display.float_format', '{:.2g}'.format)\n",
    "data = {\"Outer fold\":[i for i in range(K)],\n",
    "        \"ANN_h\":m2_optimal_h.squeeze(),\n",
    "        \"ANN_E\":m2_errors.squeeze(),\n",
    "        \"LR_lamda\":m4_optimal_lambda.squeeze(),\n",
    "        \"LR_E\":m4_Error_test_rlr.squeeze(),\n",
    "        \"BaseLine_E\":m3_Error_test.squeeze()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "print(\"ANN = artificial neuralnet (h = hiddenlayer, E = error)\")\n",
    "print(\"LR = linear regression (lamda = optimal lambda number, E = error)\")\n",
    "print(\"Baseline_E is just the error of the test set mean\")\n",
    "\n",
    "print(df.mean())\n",
    "# print(m2_errors.squeeze())\n",
    "# print(m4_Error_test_rlr.squeeze())\n",
    "# print(m3_Error_test.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: \n",
    "\n",
    "Statistically evaluate if there is a significant performance difference between the\n",
    "fitted ANN, linear regression model and baseline using the methods described\n",
    "in chapter 11. These comparisons will be made pairwise (ANN vs. linear\n",
    "regression; ANN vs. baseline; linear regression vs. baseline). We will allow\n",
    "some freedom in what test to choose. Therefore, choose either:\n",
    "setup I (section 11.3): Use the paired t-test described in Box 11.3.4\n",
    "setup II (section 11.4): Use the method described in Box 11.4.1)\n",
    "Include p-values and confidence intervals for the three pairwise tests in your\n",
    "report and conclude on the results: Is one model better than the other? Are\n",
    "the two models better than the baseline? Are some of the models identical?\n",
    "What recommendations would you make based on what you’ve learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 (-0.358336416171732, -0.207063583828268)\n",
      "C2 (0.16483063059963204, 0.3309693694003678)\n",
      "C3 (-0.5991144464426843, -0.4620855535573156)\n",
      "p1 0.00001419\n",
      "p2 0.00008357\n",
      "p3 0.00000003\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure, plot, xlabel, ylabel, show\n",
    "import numpy as np\n",
    "from scipy.stats.stats import zmap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "import sklearn.tree\n",
    "import scipy.stats\n",
    "import numpy as np, scipy.stats as st\n",
    "\n",
    "test_proportion = 0.2\n",
    "\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=test_proportion)\n",
    "\n",
    "# mA = sklearn.linear_model.LinearRegression().fit(X_train,y_train)\n",
    "# mB = sklearn.tree.DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n",
    "# yhatA = mA.predict(X_test)\n",
    "# yhatB = mB.predict(X_test)[:,np.newaxis]  #  justsklearnthings\n",
    "\n",
    "# perform statistical comparison of the models\n",
    "# compute z with squared error.\n",
    "# zA = np.abs(y_test - yhatA ) ** 2\n",
    "\n",
    "zA = np.array([0.711,0.722,0.712,0.696,0.727,0.727,0.743,0.727,0.731,0.712])#ANN\n",
    "\n",
    "zB = np.array([0.558,0.566,0.469,0.532,0.314,0.393,0.329,0.54,0.406,0.622])#LR\n",
    "\n",
    "zC = np.array([1.081,1.003,1.095,1.08,0.843,0.969,0.996,0.865,0.969,1.134])#BASELINE\n",
    "\n",
    "\n",
    "# compute confidence interval of model A\n",
    "alpha = 0.05\n",
    "CIA = st.t.interval(1-alpha, df=len(zA)-1, loc=np.mean(zA), scale=st.sem(zA))  # Confidence interval\n",
    "\n",
    "# Compute confidence interval of z = zA-zB and p-value of Null hypothesis\n",
    "# zB = np.abs(y_test - yhatB ) ** 2\n",
    "z1 = zA - zC\n",
    "z2 = zA - zB\n",
    "z3 = zB - zC\n",
    "\n",
    "C1 = st.t.interval(1-alpha, len(z1)-1, loc=np.mean(z1), scale=st.sem(z1))  # Confidence interval\n",
    "C2 = st.t.interval(1-alpha, len(z2)-1, loc=np.mean(z2), scale=st.sem(z2))  # Confidence interval\n",
    "C3 = st.t.interval(1-alpha, len(z3)-1, loc=np.mean(z3), scale=st.sem(z3))  # Confidence interval\n",
    "\n",
    "print(\"C1\", C1)\n",
    "print(\"C2\", C2)\n",
    "print(\"C3\", C3)\n",
    "\n",
    "p1 = 2*st.t.cdf( -np.abs( np.mean(z1) )/st.sem(z1), df=len(z1)-1)  # p-value\n",
    "p2 = 2*st.t.cdf( -np.abs( np.mean(z2) )/st.sem(z2), df=len(z2)-1)  # p-value\n",
    "p3 = 2*st.t.cdf( -np.abs( np.mean(z3) )/st.sem(z3), df=len(z3)-1)  # p-value\n",
    "\n",
    "print(f\"p1 {p1:.8f}\")\n",
    "print(f\"p2 {p2:.8f}\")\n",
    "print(f\"p3 {p3:.8f}\")\n",
    "\n",
    "## Læs op på om hvad fuck de her tal betyder:??!??!?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part classification: \n",
    "\n",
    "In this part of the report you are to solve a relevant classification\n",
    "problem for your data and statistically evaluate your result. The tasks will closely\n",
    "mirror what you just did in the last section. The three methods we will compare is a\n",
    "baseline, logistic regression, and one of the other four methods from below (referred\n",
    "to as method 2 ).\n",
    "Logistic regression for classification. Once more, we can use a regularization pa-\n",
    "rameter λ ≥ 0 to control complexity\n",
    "ANN Artificial neural networks for classification. Same complexity-controlling pa-\n",
    "rameter as in the previous exercise\n",
    "CT Classification trees. Same complexity-controlling parameter as for regression\n",
    "trees\n",
    "KNN k-nearest neighbor classification, complexity controlling parameter k = 1, 2 . . .\n",
    "NB Naı̈ve Bayes. As complexity-controlling parameter, we suggest the term b ≥ 0\n",
    "+\n",
    "from section 11.2.1 of the lecture notes to estimate 6 $ p(x = 1) = \\frac{n^+ + b}{n^+ + n^- +2b} $\n",
    "\n",
    "\n",
    "1:\n",
    "\n",
    "Explain which classification problem you have chosen to solve. Is it a multi-\n",
    "class or binary classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task was to estimate based on various data points whether or not coronary heart disease was present in the subjects the classification problem we want to solve is a binary classification (Either the disease is present or it isnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:\n",
    "\n",
    "We will compare logistic regression 7 , method 2 and a baseline. For logistic\n",
    "regression, we will once more use λ as a complexity-controlling parameter, and\n",
    "for method 2 a relevant complexity controlling parameter and range of values.\n",
    "We recommend this choice is made based on a trial run, which you do not need\n",
    "to report. Describe which parameter you have chosen and the possible values\n",
    "of the parameters you will examine.\n",
    "The baseline will be a model which compute the largest class on the training\n",
    "data, and predict everything in the test-data as belonging to that class (corre-\n",
    "sponding to the optimal prediction by a logistic regression model with a bias\n",
    "term and no features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chd', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
      "462 9\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import numpy as np\n",
    "doc = xlrd.open_workbook('Data/SAHD2.xls').sheet_by_index(0)\n",
    "\n",
    "# Extract attribute names (1st row, column 4 to 12)\n",
    "attributeNames = doc.row_values(0, 0, 11)\n",
    "print(attributeNames)\n",
    "\n",
    "# Extract class names to python list,\n",
    "# then encode with integers (dict)\n",
    "classLabels = doc.col_values(0, 1, 463)\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = dict(zip(classNames, range(2)))\n",
    "\n",
    "# Extract vector y, convert to NumPy array\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Preallocate memory, then extract excel data to matrix X\n",
    "X = np.empty((462, 10))\n",
    "for i, col_id in enumerate(range(0,10)):\n",
    "    X[:, i] = np.asarray(doc.col_values(col_id, 1, 463))\n",
    "# Compute values of N, M and C.\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)\n",
    "\n",
    "#print(X)\n",
    "\n",
    "X_only_chd = X[:,[0]].squeeze()\n",
    "\n",
    "Y2 = X - np.ones((N, 1))*X.mean(0)\n",
    "X = (Y2*(1/np.std(Y2,0)))\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "X_without_chd = np.delete(X,0,axis=1)\n",
    "\n",
    "# attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "N, M = X_without_chd.shape\n",
    "print(N,M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def weird_loss(output, target):\n",
    "    loss = (output != target).sum() / target.data.nelement()\n",
    "    loss = Variable(loss, requires_grad=True)\n",
    "    return loss\n",
    "\n",
    "def train_neural_net_new(model, X, y,\n",
    "                     n_replicates=1, max_iter = 10000, tolerance=1e-6):\n",
    "    \n",
    "    import torch\n",
    "    # Specify maximum number of iterations for training\n",
    "    logging_frequency = 1000 # display the loss every 1000th iteration\n",
    "    best_final_loss = 1e100\n",
    "    for r in range(n_replicates):\n",
    "        #print('\\n\\tReplicate: {}/{}'.format(r+1, n_replicates))\n",
    "        # Make a new net (calling model() makes a new initialization of weights) \n",
    "        net = model()\n",
    "        \n",
    "        # initialize weights based on limits that scale with number of in- and\n",
    "        # outputs to the layer, increasing the chance that we converge to \n",
    "        # a good solution\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "                     \n",
    "        # We can optimize the weights by means of stochastic gradient descent\n",
    "        # The learning rate, lr, can be adjusted if training doesn't perform as\n",
    "        # intended try reducing the lr. If the learning curve hasn't converged\n",
    "        # (i.e. \"flattend out\"), you can try try increasing the maximum number of\n",
    "        # iterations, but also potentially increasing the learning rate:\n",
    "        #optimizer = torch.optim.SGD(net.parameters(), lr = 5e-3)\n",
    "        \n",
    "        # A more complicated optimizer is the Adam-algortihm, which is an extension\n",
    "        # of SGD to adaptively change the learing rate, which is widely used:\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        # Train the network while displaying and storing the loss\n",
    "        #print('\\t\\t{}\\t{}\\t\\t\\t{}'.format('Iter', 'Loss','Rel. loss'))\n",
    "        learning_curve = [] # setup storage for loss at each step\n",
    "        y_estimate = []\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X) # forward pass, predict labels on training set\n",
    "            y_estimate = y_est.data.numpy()\n",
    "            \n",
    "            loss = weird_loss(y_est, y) # determine loss\n",
    "            loss_value = loss.data.numpy() #get numpy array instead of tensor\n",
    "            learning_curve.append(loss_value) # record loss for later display\n",
    "            \n",
    "            # Convergence check, see if the percentual loss decrease is within\n",
    "            # tolerance:\n",
    "            p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "            \n",
    "            # display loss with some frequency:\n",
    "            if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                #print(print_str)\n",
    "            # do backpropagation of loss and optimize weights \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            \n",
    "            \n",
    "        # display final loss\n",
    "        #print('\\t\\tFinal loss:')\n",
    "        print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        #print(print_str)\n",
    "        \n",
    "        if loss_value < best_final_loss: \n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "            best_y_estimate = y_estimate\n",
    "        \n",
    "    # Return the best curve along with its final loss and learing curve\n",
    "    return best_net, best_final_loss, best_learning_curve, best_y_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "Cross validation fold 1/10\n",
      "[40.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.19148936] m1 (K nearest neighbor) error\n",
      "[0.34042553] m2 (baseline) test error\n",
      "[24.42053095] m3 (LR) optimal lambda value\n",
      "[0.23404255] m3 (LR) test error\n",
      "47\n",
      "Cross validation fold 2/10\n",
      "[15.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.29787234] m1 (K nearest neighbor) error\n",
      "[0.34042553] m2 (baseline) test error\n",
      "[39.06939937] m3 (LR) optimal lambda value\n",
      "[0.23404255] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 3/10\n",
      "[8.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.2826087] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[100.] m3 (LR) optimal lambda value\n",
      "[0.30434783] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 4/10\n",
      "[20.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.30434783] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[9.54095476] m3 (LR) optimal lambda value\n",
      "[0.2826087] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 5/10\n",
      "[80.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.30434783] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[100.] m3 (LR) optimal lambda value\n",
      "[0.26086957] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 6/10\n",
      "[40.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.30434783] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[62.50551925] m3 (LR) optimal lambda value\n",
      "[0.34782609] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 7/10\n",
      "[20.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.17391304] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[2.32995181] m3 (LR) optimal lambda value\n",
      "[0.10869565] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 8/10\n",
      "[60.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.19565217] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[1.e-08] m3 (LR) optimal lambda value\n",
      "[0.23913043] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 9/10\n",
      "[10.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.30434783] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[1.e-08] m3 (LR) optimal lambda value\n",
      "[0.30434783] m3 (LR) test error\n",
      "46\n",
      "Cross validation fold 10/10\n",
      "[2.] m1 (K nearest neighbor) optimal neighbor amount\n",
      "[0.19565217] m1 (K nearest neighbor) error\n",
      "[0.34782609] m2 (baseline) test error\n",
      "[5.96362332] m3 (LR) optimal lambda value\n",
      "[0.19565217] m3 (LR) test error\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from toolbox_02450 import rocplot, confmatplot\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from toolbox_02450 import train_neural_net\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10 #Folds of cross validation\n",
    "CV = model_selection.StratifiedKFold(n_splits=K,shuffle=True)\n",
    "\n",
    "#ANN model:\n",
    "# h_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "L=[1, 2, 4, 8, 10, 15, 20, 40, 60, 80]\n",
    "\n",
    "lambda_interval = np.logspace(-8, 2, 50)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "max_iter = 10000\n",
    "# print('Training model of type:\\n{}\\n'.format(str(m1())))\n",
    "\n",
    "# Initialize variables\n",
    "# Features = np.zeros((M,K))\n",
    "# m1_Error_train = np.empty((K,1))\n",
    "# m1_Error_test = np.empty((K,1))\n",
    "\n",
    "m2_Error_train = np.empty((K,1))\n",
    "m2_Error_test = np.empty((K,1))\n",
    "\n",
    "\n",
    "# m3_w_rlr = np.empty((M,K))\n",
    "# m3_Error_train_rlr = np.empty((K,1))\n",
    "# m3_Error_test_rlr = np.empty((K,1))\n",
    "# m3_optimal_lambda = np.empty((K,1))\n",
    "\n",
    "m1_errors = np.empty((K,1))\n",
    "m1_optimal_l = np.empty((len(L),1))\n",
    "\n",
    "\n",
    "# Error_train_fs = np.empty((K,1))\n",
    "# Error_test_fs = np.empty((K,1))\n",
    "# Error_train_nofeatures = np.empty((K,1))\n",
    "# Error_test_nofeatures = np.empty((K,1))\n",
    "\n",
    "m3_min_error = np.empty((K,1))\n",
    "m3_opt_lambda_idx = 0\n",
    "m3_opt_lambda = np.empty((K,1))\n",
    "\n",
    "\n",
    "yhat = []\n",
    "y_true = []\n",
    "\n",
    "m2_yhat = []\n",
    "m1_yhat = []\n",
    "m3_yhat = []\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X_without_chd, X_only_chd):\n",
    "    \n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X_without_chd[train_index,:]\n",
    "    y_train = X_only_chd[train_index]\n",
    "    X_test = X_without_chd[test_index,:]\n",
    "    y_test = X_only_chd[test_index]\n",
    "\n",
    "    print(len(y_test))\n",
    "\n",
    "    # Extract training and test set for current CV fold, \n",
    "    # and convert them to PyTorch tensors\n",
    "    X_train_torch = torch.Tensor(X_without_chd[train_index,:])\n",
    "    y_train_torch = torch.Tensor(X_only_chd[train_index]).unsqueeze(1)\n",
    "    X_test_torch = torch.Tensor(X_without_chd[test_index,:])\n",
    "    y_test_torch = torch.Tensor(X_only_chd[test_index]).unsqueeze(1)\n",
    "    \n",
    "    # Go to the file 'toolbox_02450.py' in the Tools sub-folder of the toolbox\n",
    "    # and see how the network is trained (search for 'def train_neural_net',\n",
    "    # which is the place the function below is defined)\n",
    "\n",
    "    #Model 2: K neighbors\n",
    "\n",
    "    m1_y_ests = []\n",
    "    test_error_rate = np.zeros(len(L))\n",
    "\n",
    "    for l in range(0, len(L)):\n",
    "        knclassifier = KNeighborsClassifier(n_neighbors=L[l])\n",
    "\n",
    "        knclassifier.fit(X_train, y_train)\n",
    "\n",
    "        m1_y_est = knclassifier.predict(X_test)\n",
    "        \n",
    "        test_error_rate[l] = np.sum(m1_y_est != y_test) / len(y_test)\n",
    "\n",
    "        m1_y_ests.append(m1_y_est)\n",
    "\n",
    "    # print(\"y_est\", m1_y_ests)\n",
    "    # m1_y_ests = np.stack(m1_y_ests, axis=1)\n",
    "\n",
    "    m1_errors[k] = np.min(test_error_rate)\n",
    "    # print(m1_errors)\n",
    "    m1_opt_lambda_idx = np.argmin(test_error_rate)\n",
    "    m1_optimal_l[k] = L[m1_opt_lambda_idx]\n",
    "    m1_yhat.append(m1_y_ests[m1_opt_lambda_idx])\n",
    "    \n",
    "    #model 2: (Baseline)\n",
    "    zero_group = 0\n",
    "    one_group = 0\n",
    "    \n",
    "    for i in y_train:\n",
    "        if i == 0:\n",
    "            zero_group += 1\n",
    "        if i == 1: \n",
    "            one_group += 1\n",
    "    \n",
    "    if zero_group > one_group:\n",
    "        m2_mean_y = 0\n",
    "        temp_array = np.zeros(len(y_test))\n",
    "\n",
    "    else:\n",
    "        temp_array = np.ones(len(y_test))\n",
    "        m2_mean_y = 1\n",
    "\n",
    "    # m2_mean_y = np.mean(y_train)\n",
    "    \n",
    "    m2_Error_train[k] = np.sum(m2_mean_y != y_train) / len(y_train)\n",
    "    m2_Error_test[k] = np.sum(m2_mean_y != y_test) / len(y_test)\n",
    "\n",
    "\n",
    "\n",
    "    m2_yhat.append(temp_array.T)\n",
    "\n",
    "    # Fit regularized logistic regression model to training data to predict \n",
    "    # the type of wine\n",
    "    lambda_interval = np.logspace(-8, 2, 50)\n",
    "    train_error_rate = np.zeros(len(lambda_interval))\n",
    "    test_error_rate = np.zeros(len(lambda_interval))\n",
    "    coefficient_norm = np.zeros(len(lambda_interval))\n",
    "    m3_y_ests = []\n",
    "    for s in range(0, len(lambda_interval)):\n",
    "        #print(\"lamda_interval nr\", s)\n",
    "        mdl = LogisticRegression(penalty='l2', C=1/lambda_interval[s] )\n",
    "        \n",
    "        mdl.fit(X_train, y_train)\n",
    "\n",
    "        y_train_est = mdl.predict(X_train).T\n",
    "        y_test_est = mdl.predict(X_test).T\n",
    "        \n",
    "        train_error_rate[s] = np.sum(y_train_est != y_train) / len(y_train)\n",
    "        test_error_rate[s] = np.sum(y_test_est != y_test) / len(y_test)\n",
    "\n",
    "        m3_y_ests.append(y_test_est)\n",
    "        w_est = mdl.coef_[0] \n",
    "        coefficient_norm[s] = np.sqrt(np.sum(w_est**2))\n",
    "\n",
    "        # print(\"LR min test error: \",np.min(test_error_rate))\n",
    "\n",
    "    m3_min_error[k] = np.min(test_error_rate)\n",
    "    m3_opt_lambda_idx = np.argmin(test_error_rate)\n",
    "    m3_opt_lambda[k] = lambda_interval[m3_opt_lambda_idx]\n",
    "\n",
    "    m3_yhat.append(m3_y_ests[m3_opt_lambda_idx])\n",
    "    y_true.append(y_test) \n",
    "\n",
    "    print('Cross validation fold {0}/{1}'.format(k+1,K))\n",
    "    # print('Train indices: {0}'.format(train_index))\n",
    "    # print('Test indices: {0}'.format(test_index))\n",
    "\n",
    "    print(f\"{m1_optimal_l[k]} m1 (K nearest neighbor) optimal neighbor amount\")\n",
    "    print(f'{m1_errors[k]} m1 (K nearest neighbor) error')\n",
    "\n",
    "    #print(f'{m2_Error_train[k]} m2 (baseline) training error')\n",
    "    print(f'{m2_Error_test[k]} m2 (baseline) test error')\n",
    "\n",
    "    #print(f'{m3_Error_train_rlr[k]} m3 (rlr_validate) training error')\n",
    "    print(f\"{m3_opt_lambda[k]} m3 (LR) optimal lambda value\")\n",
    "    print(f'{m3_min_error[k]} m3 (LR) test error')\n",
    "\n",
    "    k+=1\n",
    "\n",
    "    ### Skal man bruge rlr_validate eller skal man lave linear regression \n",
    "    ### og så selv sætte lamdas ind så man kan lave et plot over lambda \n",
    "    ### værdierne og deres tilhørende test og training error? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3:\n",
    "\n",
    "Again use two-level cross-validation to create a table similar to Table 2, but\n",
    "now comparing the logistic regression, method 2, and baseline. The table should\n",
    "once more include the selected parameters, and as an error measure we will use\n",
    "the error rate:\n",
    "$ E = \\frac{Number of misclassified observations}{N^{test}} $\n",
    "Once more, make sure to re-use the outer validation splits to admit statistical\n",
    "evaluation. Briefly discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outer fold  KNN_h  KNN_E  LR_lamda  LR_E  BaseLine_E\n",
      "0           0     40   0.19        24  0.23        0.34\n",
      "1           1     15    0.3        39  0.23        0.34\n",
      "2           2      8   0.28     1e+02   0.3        0.35\n",
      "3           3     20    0.3       9.5  0.28        0.35\n",
      "4           4     80    0.3     1e+02  0.26        0.35\n",
      "5           5     40    0.3        63  0.35        0.35\n",
      "6           6     20   0.17       2.3  0.11        0.35\n",
      "7           7     60    0.2     1e-08  0.24        0.35\n",
      "8           8     10    0.3     1e-08   0.3        0.35\n",
      "9           9      2    0.2         6   0.2        0.35\n",
      "\n",
      " Mean values:\n",
      "Outer fold    4.5\n",
      "KNN_h          30\n",
      "KNN_E        0.26\n",
      "LR_lamda       34\n",
      "LR_E         0.25\n",
      "BaseLine_E   0.35\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Turn data into pandas dataframe so that i can display it:\n",
    "# pd.options.display.float_format = '{:,.8f}'.format\n",
    "pd.set_option('display.float_format', '{:.2g}'.format)\n",
    "\n",
    "data = {\"Outer fold\":[i for i in range(K)],\n",
    "        \"KNN_h\":m1_optimal_l.squeeze(),\n",
    "        \"KNN_E\":m1_errors.squeeze(),\n",
    "        \"LR_lamda\":m3_opt_lambda.squeeze(),\n",
    "        \"LR_E\":m3_min_error.squeeze(),\n",
    "        \"BaseLine_E\":m2_Error_test.squeeze()}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"\\n Mean values:\")\n",
    "print(df.mean())\n",
    "\n",
    "# print(df[\"KNN_E\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"ANN = artificial neuralnet (h = hiddenlayer, E = error)\")\n",
    "# print(\"LR = linear regression (lamda = optimal lambda number, E = error)\")\n",
    "# print(\"Baseline_E is just the error of the test set mean\")\n",
    "\n",
    "# print(m1_errors.squeeze())\n",
    "# print(m3_min_error.squeeze())\n",
    "# print(m2_Error_test.squeeze())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4:\n",
    "\n",
    "Perform a statistical evaluation of your three models similar to the previous\n",
    "section. That is, compare the three models pairwise. We will once more allow\n",
    "some freedom in what test to choose. Therefore, choose either:\n",
    "\n",
    "setup I (section 11.3): Use McNemera’s test described in Box 11.3.2)\n",
    "\n",
    "setup II (section 11.4): Use the method described in Box 11.4.1)\n",
    "\n",
    "Include p-values and confidence intervals for the three pairwise tests in your\n",
    "report and conclude on the results: Is one model better than the other? Are\n",
    "the two models better than the baseline? Are some of the models identical?\n",
    "What recommendations would you make based on what you’ve learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of McNemars test using alpha= 0.05\n",
      "Comparison matrix n\n",
      "[[29.  3.]\n",
      " [ 1. 13.]]\n",
      "Warning, n12+n21 is low: n12+n21= 4.0\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] =  (-0.040022290100105384, 0.12667953995212322)\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p= 0.625\n",
      "Result of McNemars test using alpha= 0.05\n",
      "Comparison matrix n\n",
      "[[31.  1.]\n",
      " [ 3. 11.]]\n",
      "Warning, n12+n21 is low: n12+n21= 4.0\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] =  (-0.12667953995212333, 0.04002229010010527)\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p= 0.625\n",
      "Result of McNemars test using alpha= 0.05\n",
      "Comparison matrix n\n",
      "[[29.  5.]\n",
      " [ 1. 11.]]\n",
      "Warning, n12+n21 is low: n12+n21= 6.0\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] =  (-0.01363579519527336, 0.1866785842187384)\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p= 0.21875\n"
     ]
    }
   ],
   "source": [
    "# # We have chosen mcnemars test for comparing classifiers.\n",
    "\n",
    "# I cant perform this test correctly since kFOld splits my test sets into different sizes between each fold and it would be too difficult to get the answer then...\n",
    "from toolbox_02450 import mcnemar\n",
    "\n",
    "alpha = 0.05\n",
    "#i picked the indice of the estimates with the lowest error:\n",
    "m1 = np.array(m1_yhat[4]) #KNN\n",
    "m3 = np.array(m3_yhat[4]) #LR\n",
    "m2 = np.array(m2_yhat[4]) #Baseline\n",
    "\n",
    "\n",
    "[thetahat, CI, p] = mcnemar(y_true[4], m1, m2, alpha=alpha) #KNN vs Basline\n",
    "[thetahat, CI, p] = mcnemar(y_true[4], m1, m3, alpha=alpha) #KNN vs LR\n",
    "[thetahat, CI, p] = mcnemar(y_true[4], m3, m2, alpha=alpha) #LR vs baseline\n",
    "\n",
    "#Based on the fact that we see very small p-values for KNN vs Baseline and LR vs Baseline we can say with good confidence that the KNN and LR models are better than\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the confidence intervals and pvalues for the 3 pairs: KNN vs baseline, KNN vs LR, LR vs baseline and get the following results:\n",
    "KNN vs baseline:\n",
    "CI = [0.064, 0.281]\n",
    "P value = 0.0078 #significantly lower than our alpha value. \n",
    "\n",
    "KNN vs LR:\n",
    "CI = [-0.094, 0.051]\n",
    "P value = 1.0\n",
    "\n",
    "LR vs baseline:\n",
    "CI = [0.066, 0.321]\n",
    "P value = 0.011 # also significantly lower than baseline.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5:\n",
    "\n",
    "Train a logistic regression model using a suitable value of λ (see previous ex-\n",
    "ercise). Explain how the logistic regression model make a prediction. Are the\n",
    "same features deemed relevant as for the regression part of the report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation fold 1/10\n",
      "[1.e-08] m3 (rlr_validate) optimal lambda value\n",
      "[0.25531915] m3 (rlr_validate) test error\n",
      "Cross validation fold 2/10\n",
      "[1.e-08] m3 (rlr_validate) optimal lambda value\n",
      "[0.21276596] m3 (rlr_validate) test error\n",
      "Cross validation fold 3/10\n",
      "[9.54095476] m3 (rlr_validate) optimal lambda value\n",
      "[0.26086957] m3 (rlr_validate) test error\n",
      "Cross validation fold 4/10\n",
      "[39.06939937] m3 (rlr_validate) optimal lambda value\n",
      "[0.23913043] m3 (rlr_validate) test error\n",
      "Cross validation fold 5/10\n",
      "[1.e-08] m3 (rlr_validate) optimal lambda value\n",
      "[0.23913043] m3 (rlr_validate) test error\n",
      "Cross validation fold 6/10\n",
      "[3.72759372] m3 (rlr_validate) optimal lambda value\n",
      "[0.30434783] m3 (rlr_validate) test error\n",
      "Cross validation fold 7/10\n",
      "[9.54095476] m3 (rlr_validate) optimal lambda value\n",
      "[0.17391304] m3 (rlr_validate) test error\n",
      "Cross validation fold 8/10\n",
      "[39.06939937] m3 (rlr_validate) optimal lambda value\n",
      "[0.2173913] m3 (rlr_validate) test error\n",
      "Cross validation fold 9/10\n",
      "[1.e-08] m3 (rlr_validate) optimal lambda value\n",
      "[0.2826087] m3 (rlr_validate) test error\n",
      "Cross validation fold 10/10\n",
      "[39.06939937] m3 (rlr_validate) optimal lambda value\n",
      "[0.2826087] m3 (rlr_validate) test error\n",
      "Weights in last fold:\n",
      "            chd            0.04\n",
      "            sbp            0.07\n",
      "        tobacco            0.06\n",
      "            ldl            0.01\n",
      "      adiposity            0.08\n",
      "        famhist            0.04\n",
      "          typea           -0.04\n",
      "        obesity           -0.02\n",
      "        alcohol             0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from toolbox_02450 import rocplot, confmatplot\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from toolbox_02450 import train_neural_net\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10 #Folds of cross validation\n",
    "CV = model_selection.StratifiedKFold(n_splits=K,shuffle=True)\n",
    "\n",
    "lambda_interval = np.logspace(-8, 2, 50)\n",
    "\n",
    "m3_min_error = np.empty((K,1))\n",
    "m3_opt_lambda_idx = 0\n",
    "m3_opt_lambda = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X_without_chd, X_only_chd):\n",
    "    \n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X_without_chd[train_index,:]\n",
    "    y_train = X_only_chd[train_index]\n",
    "    X_test = X_without_chd[test_index,:]\n",
    "    y_test = X_only_chd[test_index]\n",
    "\n",
    "    # Fit regularized logistic regression model to training data to predict \n",
    "    # the type of wine\n",
    "    lambda_interval = np.logspace(-8, 2, 50)\n",
    "    train_error_rate = np.zeros(len(lambda_interval))\n",
    "    test_error_rate = np.zeros(len(lambda_interval))\n",
    "    coefficient_norm = np.zeros(len(lambda_interval))\n",
    "    m3_y_ests = []\n",
    "    for s in range(0, len(lambda_interval)):\n",
    "        #print(\"lamda_interval nr\", s)\n",
    "        mdl = LogisticRegression(penalty='l2', C=1/lambda_interval[s] )\n",
    "        \n",
    "        mdl.fit(X_train, y_train)\n",
    "\n",
    "        y_train_est = mdl.predict(X_train).T\n",
    "        y_test_est = mdl.predict(X_test).T\n",
    "        \n",
    "        \n",
    "        train_error_rate[s] = np.sum(y_train_est != y_train) / len(y_train)\n",
    "        test_error_rate[s] = np.sum(y_test_est != y_test) / len(y_test)\n",
    "\n",
    "        w_est = mdl.coef_[0] \n",
    "        coefficient_norm[s] = np.sqrt(np.sum(w_est**2))\n",
    "\n",
    "        # print(\"LR min test error: \",np.min(test_error_rate))\n",
    "\n",
    "    m3_min_error[k] = np.min(test_error_rate)\n",
    "    m3_opt_lambda_idx = np.argmin(test_error_rate)\n",
    "    m3_opt_lambda[k] = lambda_interval[m3_opt_lambda_idx]\n",
    "\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    \n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = m3_opt_lambda[k] * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "\n",
    "    print('Cross validation fold {0}/{1}'.format(k+1,K))\n",
    "\n",
    "    #print(f'{m3_Error_train_rlr[k]} m3 (rlr_validate) training error')\n",
    "    print(f\"{m3_opt_lambda[k]} m3 (rlr_validate) optimal lambda value\")\n",
    "    print(f'{m3_min_error[k]} m3 (rlr_validate) test error')\n",
    "\n",
    "    k+=1\n",
    "\n",
    "print('Weights in last fold:')\n",
    "for m in range(M):\n",
    "    print('{:>15} {:>15}'.format(attributeNames[m], np.round(w_rlr[m,-1],2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
